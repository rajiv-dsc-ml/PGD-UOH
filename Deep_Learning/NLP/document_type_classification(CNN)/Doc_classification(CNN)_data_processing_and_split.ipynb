{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMgSstPYv0P"
      },
      "source": [
        "# Text Classification:\n",
        "\n",
        "## Data\n",
        "<pre>\n",
        "1. we have total of 20 types of documents(Text files) and total 18828 documents(text files).\n",
        "2. You can download data from this <a href='https://drive.google.com/open?id=1rxD15nyeIPIAZ-J2VYPrDRZI66-TBWvM'>link</a>, in that you will get documents.rar folder. <br>If you unzip that, you will get total of 18828 documnets. document name is defined as'ClassLabel_DocumentNumberInThatLabel'. \n",
        "so from document name, you can extract the label for that document.\n",
        "4. Now our problem is to classify all the documents into any one of the class.\n",
        "5. Below we provided count plot of all the labels in our data. \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9JpFW64w50K",
        "outputId": "4e499eb1-5cd4-401d-c02f-ab24477e885a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-02-03 13:26:21--  https://doc-00-as-docs.googleusercontent.com/docs/securesc/fev9g1e953f8o51km3me7sgfgsd0sbe5/vrscv2kc3829uh7iga13mvpjd035uloa/1643894700000/00484516897554883881/10673234880569198624/1rxD15nyeIPIAZ-J2VYPrDRZI66-TBWvM?e=download&authuser=0&nonce=1ikss80jb4s9q&user=10673234880569198624&hash=j0113valp900mqtni5o4ph45dtike74t\n",
            "Resolving doc-00-as-docs.googleusercontent.com (doc-00-as-docs.googleusercontent.com)... 74.125.128.132, 2a00:1450:4013:c02::84\n",
            "Connecting to doc-00-as-docs.googleusercontent.com (doc-00-as-docs.googleusercontent.com)|74.125.128.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested range not satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --header=\"Host: doc-00-as-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://drive.google.com/\" --header=\"Cookie: AUTH_6d57tlmd151rhrgo8ik3ogr8o2g7q8kk_nonce=1ikss80jb4s9q\" --header=\"Connection: keep-alive\" \"https://doc-00-as-docs.googleusercontent.com/docs/securesc/fev9g1e953f8o51km3me7sgfgsd0sbe5/vrscv2kc3829uh7iga13mvpjd035uloa/1643894700000/00484516897554883881/10673234880569198624/1rxD15nyeIPIAZ-J2VYPrDRZI66-TBWvM?e=download&authuser=0&nonce=1ikss80jb4s9q&user=10673234880569198624&hash=j0113valp900mqtni5o4ph45dtike74t\" -c -O 'documents.rar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqhBMdT0Zwb3"
      },
      "outputs": [],
      "source": [
        "#very important function to unzip files\n",
        "get_ipython().system_raw(\"unrar x documents.rar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbBxmi2Wp8LP"
      },
      "outputs": [],
      "source": [
        "#!cp -r /content/documents /content/backup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64U9NzWFYv0V"
      },
      "outputs": [],
      "source": [
        "### count plot of all the class labels. \n",
        "          \n",
        "dic_labels = {0:'alt.atheism' , 1:'comp.graphics', 2:'comp.os.ms-windows.misc', 3:'comp.sys.ibm.pc.hardware', 4:'comp.sys.mac.hardware', 5:'comp.windows.x', 6:'misc.forsale'\n",
        "              ,7:'rec.autos', 8:'rec.motorcycles', 9:'rec.sport.baseball', 10:'rec.sport.hockey', 11:'sci.crypt', 12:'sci.electronics', 13: 'sci.med', 14:'sci.space'\n",
        "             , 15:'soc.religion.christian', 16:'talk.politics.guns',17: 'talk.politics.mideast', 18: 'talk.politics.misc', 19: 'talk.religion.misc'}\n",
        "#comp.grap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwqYhrOP9MF0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj-KD1nqezyX",
        "outputId": "24595fab-edfd-4bc9-fa3b-abf7c2de6db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import random\n",
        "import os \n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "#lotr_pos_tags = nltk.pos_tag(words_in_lotr_quote)\n",
        "#tree = nltk.ne_chunk(lotr_pos_tags)\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"words\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "import spacy  # I have use it for chunking\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "#from nltk.tag.stanford import NERTagger\n",
        "#st = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "vaBs93bbfSkT",
        "outputId": "8ec0c0bd-7030-4fa3-b9d0-43dee774a322"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'print(preprocessed_email)\\nprint(preprocessed_subject)\\nprint(\"text is:\\n\",empty_start_text)'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#https://www.geeksforgeeks.org/how-to-iterate-over-files-in-directory-using-python/\n",
        "\n",
        "#path = \"/content/documents/alt.atheism_49960.txt\"\n",
        "#our logic is not removing punctuation \n",
        "#I have to add that logic\n",
        "\"\"\"file1 = open(path, \"a\")\n",
        "FileContent = file1.read()\n",
        "print(FileContent)\"\"\"\n",
        "\n",
        "punctuation_wo_underscore = re.sub('_', '', string.punctuation)\n",
        "\n",
        "preprocessed_email = []\n",
        "preprocessed_subject = []\n",
        "preprocessed_text = []\n",
        "debug = []\n",
        "directory =\"documents\"        \n",
        "train_y = []\n",
        "error_count =[]\n",
        "\n",
        "for i, filename in enumerate(os.scandir(directory)):\n",
        "      if filename.is_file():\n",
        "        if dic_labels[0] in str(filename):\n",
        "          train_y.append(0)\n",
        "        elif dic_labels[1] in str(filename):\n",
        "          train_y.append(1)\n",
        "        elif dic_labels[2] in str(filename):\n",
        "          train_y.append(2)\n",
        "        elif dic_labels[3] in str(filename):\n",
        "          train_y.append(3)\n",
        "        elif dic_labels[4] in str(filename):\n",
        "          train_y.append(4)\n",
        "        elif dic_labels[5] in str(filename):\n",
        "          train_y.append(5)\n",
        "        elif dic_labels[6] in str(filename):\n",
        "          train_y.append(6)\n",
        "        elif dic_labels[7] in str(filename):\n",
        "          train_y.append(7)\n",
        "        elif dic_labels[8] in str(filename):\n",
        "          train_y.append(8)\n",
        "        elif dic_labels[9] in str(filename):\n",
        "          train_y.append(9)\n",
        "        elif dic_labels[10] in str(filename):\n",
        "          train_y.append(10)\n",
        "        elif dic_labels[11] in str(filename):\n",
        "          train_y.append(11)\n",
        "        elif dic_labels[12] in str(filename):\n",
        "          train_y.append(12)\n",
        "        elif dic_labels[13] in str(filename):\n",
        "          train_y.append(13)\n",
        "        elif dic_labels[14] in str(filename):\n",
        "          train_y.append(14)\n",
        "        elif dic_labels[15] in str(filename):\n",
        "          train_y.append(15)\n",
        "        elif dic_labels[16] in str(filename):\n",
        "          train_y.append(16)\n",
        "        elif dic_labels[17] in str(filename):\n",
        "          train_y.append(17)\n",
        "        elif dic_labels[18] in str(filename):\n",
        "          train_y.append(18)\n",
        "        elif dic_labels[19] in str(filename):\n",
        "          train_y.append(19)\n",
        "        else:\n",
        "          error_count.append(str(filename))    \n",
        "\n",
        "        try:\n",
        "          with open(filename.path, 'rb') as f:\n",
        "            content = f.read().decode('ISO-8859-1')\n",
        "            #print(contents)\n",
        "            #content_new=re.sub(r'[\\w.+-]+@[\\w.+-]+', r\" \",content, flags= re.M)\n",
        "            match = re.findall(r'[\\w.+-]+@[\\w.+-]+',content)\n",
        "            #print(match)\n",
        "            ##what if there is no email in a given document?\n",
        "            ##the following code will give error at the usage of np.concatenate() which requires at least one non_zero lengthed array\n",
        "            if len(match) !=0:\n",
        "              match2 = np.array([re.sub(r'[\\w.+-]+@',\"\",s) for s in match])\n",
        "              #print(match2)\n",
        "              match3 = np.array([np.array(s.split(\".\")) for s in match2])\n",
        "              #print(match3) \n",
        "              match4 = np.concatenate(match3).ravel()\n",
        "              #print(match4)\n",
        "              indices_to_del = [i for i,v in enumerate(match4) if len(v)<=2 or v=='com'] \n",
        "              #print(indices_to_del)\n",
        "              match5 = np.delete(match4, indices_to_del)\n",
        "              #print(match5)\n",
        "              preprocessed_email.append(match5)\n",
        "              content = re.sub(r'[\\w.+-]+@[\\w.+-]+',\" \",content)\n",
        "            else:\n",
        "              lst = []\n",
        "              preprocessed_email.append(lst)\n",
        "            subject = [] # for a given document, we obtain its corresponding subject. I have taken a list for debug purpose\n",
        "            empty_start_text = \"\"\n",
        "            for _ , line in enumerate(content.split('\\n')):\n",
        "              x = re.search(r\"Subject\",line)\n",
        "              if x:\n",
        "                #print(line)\n",
        "                line = re.sub(\"\\(.+\\)\",\"\",line) # delete everything between brackets(pt7)\n",
        "                #print(line)\n",
        "                line = re.sub(\".+:\",\"\",line) #pt.4:this final 'line' variabe needs to be kept in a list. NOte that \"?\" \".\" which are there in titles can be cleaned later as they are very minor ones   \n",
        "                line = \"\".join([c for c in line if c not in punctuation_wo_underscore])\n",
        "                subject.append(line)\n",
        "                #print(repr(line))\n",
        "                line = \"\"\n",
        "                #print(repr(line)) \n",
        "              y = re.search(r\"^Write to:\",line) #pt5\n",
        "              z = re.search(r\"^From:\",line)\n",
        "              if y or z:\n",
        "                line=\"\"\n",
        "              line = re.sub(r\"\\<.*\\>\",\"\", line) #pt.6\n",
        "              line = re.sub(r\"\\(.+\\)\",\"\",line)#pt.7\n",
        "              line = re.sub(r\"[\\n|\\t|\\\\|-]\",\"\", line)#pt.8\n",
        "              line = re.sub(r\"\\w+:\",\"\",line)#pt.9\n",
        "              line= re.sub(r\"can't\", \"can not\",line)#pt.10\n",
        "              line= re.sub(r\"don't\", \"do not\",line)\n",
        "              line= re.sub(r\"'ve\", \" have\",line)\n",
        "              line = re.sub(r\"'re\",\" are\", line)\n",
        "              line = re.sub(r\"'ll\",\" will\", line)\n",
        "              line = re.sub(r\"'d\",\" would\", line)\n",
        "              line = re.sub(r\"'s\",\" is\", line)#pt.10\n",
        "              doc = nlp(line)#pt.11: chunking\n",
        "              for X in doc.ents:\n",
        "                if X.label_=='PERSON':\n",
        "                  line = line.replace(X.text,\"\")\n",
        "                if X.label_ == 'GPE':\n",
        "                  edited_place = re.sub(r\" \",\"_\",X.text)\n",
        "                  line = line.replace(X.text,edited_place)\n",
        "              line = re.sub(r'\\d',\" \",line) #pt 13:replacing any digit with space. Replacing digits like \"1234\" will lead to 4 spaces in the place of \"1234\". to mitigate that unnecessary spacing nect code\n",
        "              line = re.sub(r\" +\",' ',line)\n",
        "              #for pt.13 to 15 this link : https://learnbyexample.github.io/py_regular_expressions/groupings-and-backreferences.html\n",
        "              line = re.sub(r\"\\b([a-zA-Z0-9]+)(_)\\b\", r'\\1' ,line)             #pt14 finding \"word_\" kind and replacing with \"word\"\n",
        "              line = re.sub(r\"\\b(_)([a-zA-Z0-9]+)\\b\", r'\\2' , line)            #finding \"_word\" kind and replacing with \"word\"\n",
        "              line = re.sub(r\"\\b(_)([a-zA-Z0-9]+)(_)\\b\", r'\\2' ,line)          #finding \"_word_\" kind and replacing with \"word\"\n",
        "              line = re.sub(r\"([a-zA-Z0-9]+)(_)([a-zA-Z0-9]+)\", r'\\1 \\3' ,line) #pt.15 finding \"firstword_secondword\" kind and replacing with \"firstword secondword\"\n",
        "              #if you see point 15. it asks us to remove firstword if first word were of lengths <=2\n",
        "              #however, in pt 16, we have to anyways remove the words of lengths <=2 . So , in previous line of my code, I have not bothered to apply check on whether word just before \"_\" has lebgth <=2\n",
        "              #it will be taken care for the code logic for pt.16\n",
        "              line = re.sub(r\"\\b\\w{1,2}\\b\",\"\",line) #replacing 1 or 2 lengthed words in line with empty string. this will leave 2 consecutive spaces at some parts. pt.16\n",
        "              line = re.sub(r\"\\b\\w{15,}\\b\",\"\",line)\n",
        "              line = line.lower()\n",
        "              line = re.sub(r\"[^a-z_]\",\" \", line) #substitution with space, as directed in pt.17 is necessary . \n",
        "              line = re.sub(r\" +\",\" \",line) #removing multiple spaces with single space\n",
        "              line = \"\".join([c for c in line if c not in punctuation_wo_underscore ]) #removing all the punctuations in the line to give it a final clean\n",
        "              if _ != 0:\n",
        "                line = re.sub(r\"^\",\" \",line) #at the start of every line, put a space, so that when empty string adds two consecutive line , there remains a space between the last word of first line and the first word of the second line \n",
        "              empty_start_text = empty_start_text+ line #empty_start variable was initialised before getting inside the for loop which loops over each line in the content ofa given doc              \n",
        "\n",
        "            subject = [ re.sub(\"^ *\",\" \",i) if _ !=0 else re.sub(\"^ +\",\"\",i) for _,i in enumerate(subject)  ]\n",
        "            subject = [ re.sub(\" +$\",\"\",i) for i in subject  ] \n",
        "            subject = list(set(subject)) \n",
        "            if len(subject) > 0:\n",
        "              subject_stripped=\"\".join(subject)\n",
        "              subject = []\n",
        "              subject.append(subject_stripped)\n",
        "            preprocessed_subject.append(subject)\n",
        "            preprocessed_text.append(empty_start_text)\n",
        "            empty_start_text = \"\" #again emptying the varibale empty_start_text. This step can be avoided as before entering the for loop which loops over each line in the content ofa given doc, this variable is initialised as empty string, i.e, \"\"\n",
        "        except:\n",
        "          #print(i)\n",
        "          print(filename.path)\n",
        "          debug.append(filename.path)#to store in case a file is not processed\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\"\"\"print(preprocessed_email)\n",
        "print(preprocessed_subject)\n",
        "print(\"text is:\\n\",empty_start_text)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdjuqlSkx6qO"
      },
      "outputs": [],
      "source": [
        "\n",
        "file_name = \"preprocessed_email.pkl\"\n",
        "\n",
        "open_file = open(file_name, \"wb\")\n",
        "pickle.dump(preprocessed_email, open_file)\n",
        "open_file.close()\n",
        "\n",
        "file_name2 = \"preprocessed_subject.pkl\"\n",
        "\n",
        "open_file = open(file_name2 , \"wb\")\n",
        "pickle.dump(preprocessed_subject , open_file)\n",
        "open_file.close()\n",
        "\n",
        "file_name3 = \"preprocessed_text.pkl\"\n",
        "\n",
        "open_file = open(file_name3 , \"wb\")\n",
        "pickle.dump(preprocessed_text , open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "file_name5 = \"train_y.pkl\"\n",
        "\n",
        "open_file = open(file_name5 , \"wb\")\n",
        "pickle.dump(train_y , open_file)\n",
        "open_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA_zyG9liEZk"
      },
      "outputs": [],
      "source": [
        "open_file = open(file_name, \"rb\")\n",
        "preprocessed_email = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(file_name2, \"rb\")\n",
        "preprocessed_subject = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(file_name3, \"rb\")\n",
        "preprocessed_text = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "open_file = open(file_name5, \"rb\")\n",
        "train_y = pickle.load(open_file)\n",
        "open_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "q67oBa62iiID",
        "outputId": "fac67304-f364-4596-a3e1-c98aab75c120"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a2e127778d2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_email\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_subject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_email' is not defined"
          ]
        }
      ],
      "source": [
        "len(preprocessed_email),len(preprocessed_text), len(preprocessed_subject),len(train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpU1WlSgCM4z",
        "outputId": "5039a00a-0ca3-4759-c0fd-bf9a8194aea4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((18828, 1), (18828, 1), (18828, 1))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessed_email_ar = np.array(preprocessed_email)\n",
        "preprocessed_text_ar = np.array(preprocessed_text)\n",
        "preprocessed_subject_ar = np.array(preprocessed_subject)\n",
        "\n",
        "preprocessed_email_ar = preprocessed_email_ar[:, np.newaxis]\n",
        "preprocessed_text_ar = preprocessed_text_ar[:,np.newaxis] #or use np.reshape(preprocessed_text_ar, (-1,1))\n",
        "\n",
        "preprocessed_email_ar.shape , preprocessed_text_ar.shape ,  preprocessed_subject_ar.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-nCPzxXAMAX"
      },
      "outputs": [],
      "source": [
        "data = np.concatenate((preprocessed_text_ar,preprocessed_subject_ar, preprocessed_email_ar), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1n_RAQvLDNm",
        "outputId": "d528fc6d-52ed-417e-d699-bbaf1b05a963"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "corpus = [] \n",
        "count = 0\n",
        "for i in data:\n",
        "  #print(i)\n",
        "  lst_text = i[0].split(' ')\n",
        "  filter_lst = list(filter(lambda x : x!='' ,lst_text))\n",
        "  lst_sub = i[1].split(' ')\n",
        "  lst_email = list(i[2])\n",
        "  composite_lst = filter_lst + lst_sub + lst_email\n",
        "  if '_' in composite_lst:\n",
        "    count+=1\n",
        "  corpus.append(composite_lst)\n",
        "print(count) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K8teLSc-Wh_",
        "outputId": "aa17c508-ea61-4044-b819-bf05ea93eb60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "corpus_ar = np.array(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOeE5nWH9eRi"
      },
      "outputs": [],
      "source": [
        "array = np.array(range(len(corpus)))  #array is indices in corpus list\n",
        "#print(array)\n",
        "random.Random(0).shuffle(array)\n",
        "#print(array)\n",
        "corpus_train=corpus_ar[array[:int(len(corpus)*0.75)]]\n",
        "corpus_test = corpus_ar[array[int(len(corpus)*0.75):]]\n",
        "\n",
        "train_y = np.array(train_y)\n",
        "\n",
        "y_train = train_y[array[:int(len(corpus)*0.75)]]\n",
        "y_test = train_y[array[int(len(corpus)*0.75):]]\n",
        "\n",
        "test_size = len(corpus_test)\n",
        "second_array = np.array(range(test_size))\n",
        "random.Random(4).shuffle(second_array)\n",
        "corpus_cv = corpus_test[second_array[:int(test_size*0.6)]]\n",
        "corpus_test = corpus_test[second_array[int(test_size*0.6):]]\n",
        "\n",
        "y_cv = y_test[second_array[:int(test_size*0.6)]]\n",
        "y_test = y_test[second_array[int(test_size*0.6):]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2LQCi1jbGtZ",
        "outputId": "18d2c3c9-6671-40bd-8340-856c488d9312"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14121, 14121, 2824, 2824, 1883, 1883)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(corpus_train), len(y_train), len(corpus_cv), len(y_cv), len(corpus_test), len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLbFQn39878y",
        "outputId": "0f2f14b7-9235-4dd3-fa39-6fd27469526c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ozkCL9SfzYw9-BAWb4UFzV_stN8tJsdm\n",
            "To: /content/glove_vectors\n",
            "100% 128M/128M [00:01<00:00, 64.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1ozkCL9SfzYw9-BAWb4UFzV_stN8tJsdm  #downloading glove vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkfN59hW7aMV"
      },
      "outputs": [],
      "source": [
        "with open('glove_vectors', 'rb') as f:\n",
        "  glove =  pickle.load(f)\n",
        "  glove_words = set(glove.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoITs8GOtyAz",
        "outputId": "1f95f2d0-0a82-466b-c1fa-571cbe046960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "950 9093\n"
          ]
        }
      ],
      "source": [
        "#finding 98%ile max length of any sentence existing in corpus_train as 99%ile was giving memory failure\n",
        "lengths_of_sentences = [len(i) for i in corpus_train ]\n",
        "#99%ile\n",
        "lengths_of_sentences.sort()\n",
        "max_len_99 = int(np.percentile(lengths_of_sentences, 98))\n",
        "max_len = np.percentile(lengths_of_sentences,100 )\n",
        "print(max_len == max(lengths_of_sentences))\n",
        "print(int(max_len_99),  int(max_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XjPokGfvRBs",
        "outputId": "a6b5533d-d217-496b-8313-4f0cbc0ac99a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max length is,  950\n",
            "max length is,  950\n",
            "max length is,  950\n"
          ]
        }
      ],
      "source": [
        "#now, we will crop the sentences whose lengths exceed 2422\n",
        "#note that I will not see X_test \n",
        "#and for now, will modify X_cv according to max_len995 which is calculated over X_train\n",
        "#note that one document/sentence in corpus_train , has come from joining 3 columns in the order=> text, subject, email; maximum space is consumed by text \n",
        "#so, i will crop from the left side to cut away some part of text and preserve that right side.\n",
        "#suppose if we decide to throw away the right part for the documents whose length exceed 99.5%ile value, then it basically means, we threw away the limited info about\n",
        "#email and prefered the info about text in the sentence , which was already too large in the first place. It might very wel be more than needed.\n",
        "def cropping(x): #give corpus_train, corpus_cv, corpus_test like objects and get cropped counterparts\n",
        "  print(\"max length is, \", max_len_99)\n",
        "  cropped_corpus = []\n",
        "  for doc in x:\n",
        "    if len(doc) > max_len_99:\n",
        "      doc = doc[len(doc)- max_len_99 : ] #in case you don't want to change in the same list, del doc[: len(doc)-max_len_992]\n",
        "    cropped_corpus.append(doc)\n",
        "  return cropped_corpus\n",
        "\n",
        "\n",
        "cropped_corpus_train = cropping(corpus_train)\n",
        "cropped_corpus_cv = cropping(corpus_cv)\n",
        "cropped_corpus_test = cropping(corpus_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWeU5fV27dL7",
        "outputId": "49e23bdc-6a48-4fa5-9e3e-575f68c5bf23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14121"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(cropped_corpus_train) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqiKYwh5Mj37"
      },
      "outputs": [],
      "source": [
        "#converting train documents into numbers .\n",
        "\n",
        "words_train = [] #this wil be used to covert cv and test data\n",
        "train_num_docs=[]\n",
        "document =[]\n",
        "\n",
        "#print(max_len)\n",
        "for doc in cropped_corpus_train:\n",
        "  for w in doc:\n",
        "    if w in glove_words:\n",
        "      words_train.append(w)\n",
        "      word_vector = glove[w].astype('float32')\n",
        "      document.append(word_vector)  \n",
        "\n",
        "  train_num_docs.append(document)\n",
        "  document = [] #emptying document list\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYwDt5-3pSXo"
      },
      "outputs": [],
      "source": [
        "filename_w = \"words_train.pkl\"\n",
        "open_file = open(filename_w, 'wb')\n",
        "pickle.dump(words_train, open_file)\n",
        "open_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7R8qTYD01xM"
      },
      "outputs": [],
      "source": [
        "def text_to_num( cropped_corpus, words = words_train):\n",
        "  print(\"len of words_train\", len(words))\n",
        "  num_docs = []\n",
        "  document = []\n",
        " \n",
        "  for doc in cropped_corpus:\n",
        "    for w in doc:\n",
        "      if w in words : #note that words contain train_words, here which were already checked to have glove vwctors\n",
        "        vec = glove[w].astype('float32')\n",
        "        document.append(vec)\n",
        "    num_docs.append(document)\n",
        "    document = []\n",
        "  \n",
        "  return num_docs\n",
        "\n",
        "def padding(num_docs_, lst = np.array([0.0]*300, dtype ='float32')):\n",
        "  print(\"max_len_99:\", max_len_99)\n",
        "  for doc in num_docs_:\n",
        "    while len(doc) < max_len_99:\n",
        "      doc.append(lst)\n",
        "  return num_docs_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kLZcF5u2yCJ",
        "outputId": "d5e8e2b3-fbbc-468d-aa5d-54bb692db730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_len_99: 950\n"
          ]
        }
      ],
      "source": [
        "train_X = padding(train_num_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRrwGS7VQhtc"
      },
      "outputs": [],
      "source": [
        "train_X_ar = np.array(train_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3nD56DpYAjc",
        "outputId": "3b088f2f-2e37-4486-ccce-271b0dbcd0fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len of words_train 2122105\n",
            "max_len_99: 950\n",
            "len of words_train 2122105\n",
            "max_len_99: 950\n"
          ]
        }
      ],
      "source": [
        "cropped_cv_data = text_to_num(cropped_corpus_cv)\n",
        "padded_cropped_cv_data = padding(cropped_cv_data)\n",
        "\n",
        "cropped_test_data = text_to_num(cropped_corpus_test)\n",
        "padded_cropped_test_data = padding(cropped_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###the following two blocks are the final steps of this notebok.\n",
        "###NOte that this notebook aim was to process text data and to make it into a form amenable to modelling\n",
        "###So, finally , I have three matrices corresponding to train, validation and test.\n",
        "###Note that issues like data leak were taken into full account while data splitting in train, test, and validation\n",
        "### nature of split: random.\n",
        "### words used in training_corpus were collected and according to them only , the corpuses of validation and test were modified"
      ],
      "metadata": {
        "id": "TwJtsr9kSM9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_cv = np.array(padded_cropped_cv_data) \n",
        "X_cv.shape \n",
        "np.save(\"X_cv\", X_cv) \n",
        "X_test = np.array(padded_cropped_test_data) \n",
        "np.save(\"X_test\" , X_test)\n",
        "\n",
        "np.save(\"X_tain\", train_X_ar) "
      ],
      "metadata": {
        "id": "dojQQa2UQyX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY9rC0X-YwSa"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(y_train)\n",
        "np.save(\"y_train\", y_train)\n",
        "\n",
        "y_cv = np.array(y_cv)\n",
        "np.save(\"y_cv\", y_cv)\n",
        "\n",
        "y_test = np.array(y_test)\n",
        "np.save(\"y_test\", y_test)\n",
        "\n",
        "#note that I have carefully downloaded them to my drive and I have used them for modelling in my next notebook which is also shared in the assignment section\n",
        "#Reason of such : It was fairly long assignmnet. \n",
        "#I didnot want to mix two crucial steps of Machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The below code blocks were just experimentation and debugging at the time of preprocessing and data preparation"
      ],
      "metadata": {
        "id": "uknflBoRSC2J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fRUmDpcaYR",
        "outputId": "1aeaa95e-cafc-4c4e-bcd2-8791a05b2c40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14121, 1406, (300,))"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_num_docs), len(train_num_docs[456]), train_num_docs[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbyCDLi3zxyR",
        "outputId": "7bdc2518-6c23-4623-8b64-fef77fec0738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2204118\n"
          ]
        }
      ],
      "source": [
        "count_non_300 = 0\n",
        "for i in X_train:\n",
        "  for j in i:\n",
        "    if len(j) ==300:\n",
        "      count_non_300 +=1\n",
        "  \n",
        "  \n",
        "print(count_non_300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85q64Xs40cFo",
        "outputId": "d9cefa32-5d3d-43f3-8b94-7b2306a52aca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1406,)"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI0mECICneUz"
      },
      "outputs": [],
      "source": [
        "train_data = np.array(train_num_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOcHdm0HneYA"
      },
      "outputs": [],
      "source": [
        "train_data = np.array( [ np.array(x) for x in train_num_docs ] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgbWFAJBYLS6",
        "outputId": "b7ed59c5-c220-4a7e-8370-efe9736e3c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0\n"
          ]
        }
      ],
      "source": [
        "count_less=0\n",
        "count_more = 0\n",
        "\n",
        "for i in train_num_docs:\n",
        "  if len(i)<max_len:\n",
        "    count_less+=1\n",
        "  elif len(i) > max_len:\n",
        "    count_more+=1\n",
        "\n",
        "print(count_less, count_more)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "LOfxAImalH4S",
        "outputId": "8b47d56e-b933-4694-a4cc-94f9360c672e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-00331279e63f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_num_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "ddHPPondE3Cs",
        "outputId": "8931e480-bdfa-4fa1-920c-e3ed1cc673c6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-684b73b13302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_num_docs_ar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_num_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_num_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#train_num_docs.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "train_num_docs_ar = np.array(train_num_docs[:int(len(train_num_docs)*0.25)])\n",
        "#train_num_docs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxSoQGpOEaEB",
        "outputId": "6a0e8ffd-0914-44f4-e67d-f58d7ca267a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.float64"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "N649BPHHsr9O",
        "outputId": "30366841-0164-43a0-c629-780b05842031"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2afb651ee1ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#look[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlook\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#print(filename.path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "#look[0]\n",
        "filename = debug_doc[look[0]]\n",
        "\n",
        "if filename.is_file():\n",
        "        #print(filename.path)\n",
        "        \n",
        "          with open(filename.path, 'rb') as f:\n",
        "            content = f.read().decode('utf-8')\n",
        "            for line in content.split('\\n'):\n",
        "              x = re.search(\"Subject\",line)\n",
        "              if x:\n",
        "                print(line)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFmf9Ik4umni",
        "outputId": "418e05cc-f602-4275-e7b4-a97e194f5814"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['From Israeli press Madness',\n",
              " ' Interview with soldiers who served in the Duvdevan',\n",
              " ' From Israeli press Madness']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "debug_preprocessed_sub[look[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peeSMn_wZ1rY"
      },
      "outputs": [],
      "source": [
        "path = \"documents/misc.forsale_76395.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2PQGJsSZpUQ",
        "outputId": "a2b9fd86-2830-44d6-b406-1e7b20af0621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am snoop dog I make rap Eminem is my friend \n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"I am snoop dog.\n",
        "I make rap.\n",
        "Eminem is my friend.\n",
        "\"\"\"\n",
        "lst_ = []\n",
        "emp =\"\"\n",
        "for _,line in enumerate(text.split('\\n')):\n",
        "  line = re.sub(r'\\.','',line)\n",
        "  line = re.sub(r'\\n', '', line)\n",
        "  if _ !=0:\n",
        "    line = re.sub(r'^',' ', line)\n",
        "  emp = emp+line\n",
        "lst_.append(emp)\n",
        "print(emp)\n",
        "#print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njNX2BTpWs-k",
        "outputId": "ffbc7461-cb0a-42d8-bb2a-8714bd29bf0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['jkldddf' 'dsggdsk' 'sjfaj' 'asdjffj' 'asf' 'asg']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1520"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "match2 = [\"jkldddf.dsggdsk\",\"sjfaj.asdjffj\",\"asf.asg\"]\n",
        "match3 = np.array([np.array(s.split(\".\")) for s in match2])\n",
        "match4 = np.concatenate(match3).ravel()\n",
        "print(match4)\n",
        "len(preprocessed_email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-0cOQVjhyFf",
        "outputId": "d0b715f5-c4f1-4d99-cc7c-40fb0f09c6ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " an _apple apple_  _apple_ apple_byte d_apple @#\n",
            " an _apple apple _apple_ apple_byte d_apple @#\n",
            " an apple apple _apple_ apple_byte d_apple @#\n",
            " an apple apple apple apple_byte d_apple @#\n",
            " an apple apple apple apple byte d apple @#\n",
            "finally:   an apple apple apple apple byte d apple @#\n",
            "finally:   an apple apple apple apple byte d apple   \n",
            "  apple apple apple apple byte  apple   \n"
          ]
        }
      ],
      "source": [
        "text = \" an _apple apple_  _apple_ apple_byte d_apple @#\"\n",
        "\n",
        "text = re.sub(r\"\\d\",\" \", text)\n",
        "print(text)\n",
        "text = re.sub(r' +',' ',text)\n",
        "text = re.sub(r\"\\b([a-zA-Z0-9]+)(_)\\b\", r'\\1' ,text)\n",
        "print(text)\n",
        "text = re.sub(r\"\\b(_)([a-zA-Z0-9]+)\\b\", r'\\2' ,text)\n",
        "print(text)\n",
        "text = re.sub(r\"\\b(_)([a-zA-Z0-9]+)(_)\\b\", r'\\2' ,text)\n",
        "print(text)\n",
        "text = re.sub(r\"([a-zA-Z0-9]+)(_)([a-zA-Z0-9]+)\", r'\\1 \\3' ,text)\n",
        "print(text)\n",
        "\"\"\"matches = re.compile(r\"\\b([a-zA-Z0-9]+)(_)\\b\").finditer(text)\n",
        "for match in matches:\n",
        "  text = text.replace(match.group(0),match.group(1))\"\"\"\n",
        "\n",
        "\"\"\"matches2 = re.compile(r\"\\b(_)([a-zA-Z0-9]+)\\b\").finditer(text)\n",
        "for match in matches2: \n",
        "  text = text.replace(match.group(0),match.group(2))\n",
        "matches3 = re.compile(r\"\\b(_)([a-zA-Z0-9]+)(_)\\b\").finditer(text)\n",
        "for match in matches3:\n",
        "  text = text.replace(match.group(0),match.group(2))\n",
        "matches4 = re.compile(r\"([a-zA-Z0-9]+)(_)([a-zA-Z0-9]+)\").finditer(text)   #finding \"firstword_secondword\" kinds \n",
        "print(\"this: \",text)\n",
        "for match in matches4:\n",
        "  print(\"here: \",match.group(0))\n",
        "  text = text.replace(match.group(0),match.group(3))\n",
        "print(\"finally: \",text)\"\"\"\n",
        "print(\"finally: \",text)\n",
        "text = re.sub(r\"[^\\w]\",\" \", text)\n",
        "\n",
        "print(\"finally: \",text)\n",
        "print(re.sub(r\"\\b\\w{1,2}\\b\",\"\",text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PkhuwTbf9Ox",
        "outputId": "a5cf2fb5-8195-4be0-ddba-981d8ebd21d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adsc\n"
          ]
        }
      ],
      "source": [
        "print(\"ADSC\".lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Bsws77fxYVHt",
        "outputId": "e5168e67-90f3-46ba-ffb3-0d88a1df37b3"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-f7dea5b6307f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Your UTF-8 output file.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.decode('ISO-8859-1'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[\\w.+-]+@[\\w.+-]+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, r\" \",line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
          ]
        }
      ],
      "source": [
        "pattern = re.compile(r\"Subject:\\s?\\w*:([\\w+\\s?]+)\")\n",
        "text =\"\"\n",
        "with open('/content/Your UTF-8 output file.txt', 'rb+') as f:\n",
        "  matches = pattern.finditer(f.read())#.decode('ISO-8859-1'))\n",
        "  for line in f:\n",
        "    x = re.search(r'[\\w.+-]+@[\\w.+-]+',line)#, r\" \",line)\n",
        "    if x:\n",
        "      #line = re.sub(r'[\\w.+-]+@[\\w.+-]+', r\"abcs\",line)\n",
        "      #text+=new_line\n",
        "      print(line)\n",
        "\n",
        "\n",
        "#text = bytes(text, 'utf-8')\n",
        "\n",
        "#with open(/content/Your UTF-8 output file.txt, 'wb') as f:\n",
        "#  f.write(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPq-Ul-tj9XD",
        "outputId": "c17ab7b9-88b0-4a2f-c442-7d1cda21ab73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "' Please Recommend 3D Graphics Library For Mac.'\n",
            "  >  >>> Can you please offer some recommendations?>>It is really not that hard to do.  There are books out there which explain>everything, and the basic 3D functions, translation, rotation, shading, and>hidden line removal are pretty easy.  I wrote a program in a few weeks witht>he help of a book, and would be happy to give you my source.I think he wanted to avoid reinventing the wheel.I would suggest that you take your code, and submit it tocomp.sys.mac.binaries to be distributed . Many folks, myself included, would enjoy the extra code.>Also, Quickdraw has a lot of 3D functions built in, and Think pascal>can access them, and I would expect that THINK C could as well.  If you can>find out how to use the Quickdraw graphics library, it would be an excellent>choice, since it has a lot of stuff, and is built into the Mac, so should be>fast.Just to clarify, the 3D routines that are mentioned in various placeson the mac are in a libray, not the ROM of the mac.  A few years ago beforeI knew anything about implementing graphics, I came across a demo of theApple GrafSys3D library and it actually did a lot.  However, it is quitelimited in the sense that it is a lowlevel 3D library; your code still hasto plot individual points, draw each line, etc ad nauseum.  It has nothingon GL, for example, where you can handle objects.Other things to consider when talking about Apple is old 3D GrafSys * Unsupported;  never was and no plans exist to do so in the future* Undocumented; unless you call header files documentation...If one knows something about graphics, you could probably figure it out,but I would assume there is better software available that gives betteroutput and is, at the same time, programmatically nicer (i.e. easier toprogram).Just my 2% taxBrent\n"
          ]
        }
      ],
      "source": [
        "c=0\n",
        "text_to_search3 = \"\"\"\n",
        "From: Ivanov Sergey <serge@argus.msk.su>\n",
        "Subject: Re: Re: VGA 640x400 graphics mode\n",
        "\n",
        "> My 8514/a VESA TSR supports this\n",
        "\n",
        " Can You report CRT and other register state in this mode ?\n",
        " Thank's.\n",
        "\n",
        "        Serge Ivanov (serge@argus.msk.su)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "text_to_search4 =\"\"\"\n",
        "From: davidr@rincon.ema.rockwell.com (David J. Ray)\n",
        "Subject: Re: Fractals? what good are they?\n",
        "\n",
        "In regards to fractal commpression, I have seen 2 fractal compressed \"movies\".\n",
        "They were both fairly impressive.  The first one was a 64 gray scale \"movie\" of\n",
        "Casablanca, it was 1.3MB and had 11 minutes of 13 fps video.  It was a little\n",
        "grainy but not bad at all.  The second one I saw was only 3 minutes but it\n",
        "had 8 bit color with 10fps and measured in at 1.2MB.\n",
        "\n",
        "I consider the fractal movies a practical thing to explore.  But unlike many \n",
        "other formats out there, you do end up losing resolution.  I don't know what\n",
        "kind of software/hardware was used for creating the \"movies\" I saw but the guy\n",
        "that showed them to me said it took 5-15 minutes per frame to generate.  But as\n",
        "I said above playback was 10 or more frames per second.  And how else could you\n",
        "put 11 minutes on one floppy disk?\n",
        "\n",
        "davidr@rincon.ema.rockwell.com\n",
        "My opinions are my own except where they are shared by others in which case I \n",
        "will probably change my mind.\n",
        "\n",
        "\"\"\"\n",
        "text_to_search5 =\"\"\"\n",
        "From: brentb@tamsun.tamu.edu (Brent)\n",
        "Subject: Re: Please Recommend 3D Graphics Library For Mac.\n",
        "\n",
        "tsa@cellar.org (The Silent Assassin) writes:\n",
        ">rgc3679@bcstec.ca.boeing.com (Robert G. Carpenter) writes:\n",
        ">\n",
        ">> Can you please offer some recommendations?\n",
        ">\n",
        ">It's really not that hard to do.  There are books out there which explain\n",
        ">everything, and the basic 3D functions, translation, rotation, shading, and\n",
        ">hidden line removal are pretty easy.  I wrote a program in a few weeks witht\n",
        ">he help of a book, and would be happy to give you my source.\n",
        "\n",
        "I think he wanted to avoid reinventing the wheel.\n",
        "I would suggest that you take your code, and submit it to\n",
        "comp.sys.mac.binaries to be distributed (including to the ftp sites). \n",
        "Many folks, myself included, would enjoy the extra code.\n",
        "\n",
        ">\tAlso, Quickdraw has a lot of 3D functions built in, and Think pascal\n",
        ">can access them, and I would expect that THINK C could as well.  If you can\n",
        ">find out how to use the Quickdraw graphics library, it would be an excellent\n",
        ">choice, since it has a lot of stuff, and is built into the Mac, so should be\n",
        ">fast.\n",
        "\n",
        "Just to clarify, the 3D routines that are mentioned in various places\n",
        "on the mac are in a libray, not the ROM of the mac.  A few years ago before\n",
        "I knew anything about implementing graphics, I came across a demo of the\n",
        "Apple GrafSys3D library and it actually did a lot.  However, it is quite\n",
        "limited in the sense that it's a low-level 3D library; your code still has\n",
        "to plot individual points, draw each line, etc ad nauseum.  It has nothing\n",
        "on GL, for example, where you can handle objects.\n",
        "\n",
        "Other things to consider when talking about Apple's old 3D GrafSys library:\n",
        "\n",
        "* Unsupported;  never was and no plans exist to do so in the future\n",
        "\n",
        "* Undocumented; unless you call header files documentation...\n",
        "\n",
        "If one knows something about graphics, you could probably figure it out,\n",
        "but I'd assume there's better software available that gives better\n",
        "output and is, at the same time, programmatically nicer (i.e. easier to\n",
        "program).\n",
        "\n",
        "Just my 2% tax\n",
        "\n",
        "-Brent\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "empty_start_text=\"\"\n",
        "#here, I need to open a particular text doc in read mode\n",
        "for line in text_to_search5.split(\"\\n\"): #analysing each line\n",
        "  line = re.sub(r'[\\w.+-]+@[\\w.+-]+',\"\",line) #there are more to it, whose logic is written above\n",
        "  x = re.search(r\"Subject\",line)\n",
        "  if x:\n",
        "    #print(line)\n",
        "    line = re.sub(\"\\(.+\\)\",\"\",line) # delete everything between brackets(pt7)\n",
        "    #print(line)\n",
        "    line = re.sub(\".+:\",\"\",line) #pt.4:this final 'line' variabe needs to be kept in a list. NOte that \"?\" \".\" which are there in titles can be cleaned later as they are very minor ones   \n",
        "    \n",
        "    #print(repr(line))\n",
        "    line = \"\"\n",
        "    #print(repr(line)) \n",
        "  y = re.search(r\"^Write to:\",line) #pt5\n",
        "  z = re.search(r\"^From:\",line)\n",
        "  if y or z:\n",
        "    line=\"\"\n",
        "  line = re.sub(r\"\\<.*\\>\",\"\", line) #pt.6\n",
        "  line = re.sub(r\"\\(.+\\)\",\"\",line)\n",
        "  line = re.sub(r\"[\\n|\\t|\\\\|-]\",\"\", line)\n",
        "  line = re.sub(r\"\\w+:\",\"\",line)\n",
        "  line= re.sub(r\"can't\", \"can not\",line)\n",
        "  line= re.sub(r\"don't\", \"do not\",line)\n",
        "  line= re.sub(r\"'ve\", \" have\",line)\n",
        "  line = re.sub(r\"'re\",\" are\", line)\n",
        "  line = re.sub(r\"'ll\",\" will\", line)\n",
        "  line = re.sub(r\"'d\",\" would\", line)\n",
        "  line = re.sub(r\"'s\",\" is\", line)\n",
        "  \n",
        "  #words_in_line = word_tokenize(line)\n",
        "  \n",
        "#lotr_pos_tags = nltk.pos_tag(words_in_lotr_quote)\n",
        "#tree = nltk.ne_chunk(lotr_pos_tags)\n",
        "print(empty_start_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT7Fl3u9jX_1",
        "outputId": "b01c9949-a9b5-442b-d49c-2f7267613c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NE Rajiv/NNP)\n",
            "  is/VBZ\n",
            "  something/NN\n",
            "  but/CC\n",
            "  his/PRP$\n",
            "  luck/NN\n",
            "  is/VBZ\n",
            "  bad/JJ)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"for line in new_text_to_search5.split(\"\\n\"):\n",
        "  words = word_tokenize(line)\n",
        "  tags = nltk.pos_tag(words)\n",
        "  tree = nltk.ne_chunk(tags)\n",
        "  print(tree)\"\"\"\n",
        "def extract_ne(quote):\n",
        "     words = word_tokenize(quote)\n",
        "     tags = nltk.pos_tag(words)\n",
        "     tree = nltk.ne_chunk(tags, binary=True)\n",
        "     return set(\n",
        "         \" \".join(i[0] for i in t)\n",
        "         for t in tree\n",
        "         if hasattr(t, \"label\") and t.label() == \"NE\"\n",
        "     )\n",
        "\n",
        "#extract_ne(new_text_to_search5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for sent in nltk.sent_tokenize(\"Rajiv is something but his luck is bad\"):\n",
        "  tokens = nltk.tokenize.word_tokenize(sent)\n",
        "  tags =  nltk.pos_tag(tokens)\n",
        "  nouns = nltk.ne_chunk(tags,binary=True)\n",
        "  print(nouns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeMCAUbnrZlk",
        "outputId": "c471fde9-9e85-4f42-bd3c-6d4a8708fff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "len(debug_doc), len(debug_preprocessed_sub)\n",
        "count = 0\n",
        "look = []\n",
        "for ind,each_sub in enumerate(debug_preprocessed_sub):\n",
        "  if len(each_sub)==2:\n",
        "    count+=1    \n",
        "  else:\n",
        "    look.append(ind)\n",
        "   \n",
        "\n",
        "print(count)\n",
        "print(count == len(debug_preprocessed_sub))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LWHEmxeJ_KN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD5OyBDXKVrf",
        "outputId": "b63b4e16-cb1f-414c-c168-21140de3f875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " lives in New_York.\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "Srikanth Verma lives in New York.\n",
        "He is a great teacher.\n",
        "\"\"\"\n",
        "\n",
        "for line in text.split(\"\\n\"):\n",
        "  doc = nlp(line)\n",
        "  for X in doc.ents:\n",
        "    if X.label_=='PERSON':\n",
        "      line = line.replace(X.text,\"\")\n",
        "      #print(line)\n",
        "    if X.label_ == 'GPE':\n",
        "      edited_place = re.sub(r\" \",\"_\",X.text)\n",
        "      line = line.replace(X.text,edited_place)\n",
        "      print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orhFOVWWiK2A"
      },
      "outputs": [],
      "source": [
        "quote = \"\"\"\n",
        "Men like Schiaparelli watched the red planetit is odd, by-the-bye, that\n",
        "for countless centuries Mars has been the star of warbut failed to\n",
        "interpret the fluctuating appearances of the markings they mapped so well.\n",
        "All that time the Martians must have been getting ready.\n",
        "During the opposition of 1894 a great light was seen on the illuminated\n",
        "part of the disk, first at the Lick Observatory, then by Perrotin of Nice,\n",
        "and then by other observers. English readers heard of it first in the\n",
        "issue of Nature dated August 2.\"\"\"\n",
        "\n",
        "words_in_quote = word_tokenize(quote)\n",
        "ta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_4DgH9UUun7",
        "outputId": "52b1d277-8ffc-42d9-f762-e4b5016337b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "From: brentb@tamsun.tamu.edu (Brent)\n",
            "Subject: Re: Please Recommend 3D Graphics Library For Mac.\n",
            "\n",
            "tsa@cellar.org (The Silent Assassin) writes:\n",
            ">rgc3679@bcstec.ca.boeing.com (Robert G. Carpenter) writes:\n",
            ">\n",
            ">> Can you please offer some recommendations?\n",
            ">\n",
            ">It's really not that hard to do.  There are books out there which explain\n",
            ">everything, and the basic 3D functions, translation, rotation, shading, and\n",
            ">hidden line removal are pretty easy.  I wrote a program in a few weeks witht\n",
            ">he help of a book, and would be happy to give you my source.\n",
            "\n",
            "I think he wanted to avoid reinventing the wheel.\n",
            "I would suggest that you take your code, and submit it to\n",
            "comp.sys.mac.binaries to be distributed (including to the ftp sites). \n",
            "Many folks, myself included, would enjoy the extra code.\n",
            "\n",
            ">\tAlso, Quickdraw has a lot of 3D functions built in, and Think pascal\n",
            ">can access them, and I would expect that THINK C could as well.  If you can\n",
            ">find out how to use the Quickdraw graphics library, it would be an excellent\n",
            ">choice, since it has a lot of stuff, and is built into the Mac, so should be\n",
            ">fast.\n",
            "\n",
            "Just to clarify, the 3D routines that are mentioned in various places\n",
            "on the mac are in a libray, not the ROM of the mac.  A few years ago before\n",
            "I knew anything about implementing graphics, I came across a demo of the\n",
            "Apple GrafSys3D library and it actually did a lot.  However, it is quite\n",
            "limited in the sense that it's a low-level 3D library; your code still has\n",
            "to plot individual points, draw each line, etc ad nauseum.  It has nothing\n",
            "on GL, for example, where you can handle objects.\n",
            "\n",
            "Other things to consider when talking about Apple's old 3D GrafSys library:\n",
            "\n",
            "* Unsupported;  never was and no plans exist to do so in the future\n",
            "\n",
            "* Undocumented; unless you call header files documentation...\n",
            "\n",
            "If one knows something about graphics, you could probably figure it out,\n",
            "but I'd assume there's better software available that gives better\n",
            "output and is, at the same time, programmatically nicer (i.e. easier to\n",
            "program).\n",
            "\n",
            "Just my 2% tax\n",
            "\n",
            "-Brent\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text_to_search5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlOQLc6CTkYN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mK4TJOFYv0h"
      },
      "source": [
        "## Assignment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlqYFVI3Yv0k"
      },
      "source": [
        "#### sample document\n",
        "<pre>\n",
        "<font color='blue'>\n",
        "Subject: A word of advice\n",
        "From: jcopelan@nyx.cs.du.edu (The One and Only)\n",
        "\n",
        "In article < 65882@mimsy.umd.edu > mangoe@cs.umd.edu (Charley Wingate) writes:\n",
        ">\n",
        ">I've said 100 times that there is no \"alternative\" that should think you\n",
        ">might have caught on by now.  And there is no \"alternative\", but the point\n",
        ">is, \"rationality\" isn't an alternative either.  The problems of metaphysical\n",
        ">and religious knowledge are unsolvable-- or I should say, humans cannot\n",
        ">solve them.\n",
        "\n",
        "How does that saying go: Those who say it can't be done shouldn't interrupt\n",
        "those who are doing it.\n",
        "\n",
        "Jim\n",
        "--\n",
        "Have you washed your brain today?\n",
        "</font>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAR5HoR1Yv0m"
      },
      "source": [
        "### Preprocessing:\n",
        "<pre>\n",
        "useful links: <a href='http://www.pyregex.com/'>http://www.pyregex.com/</a>\n",
        "\n",
        "<font color='blue'><b>1.</b></font> Find all emails in the document and then get the text after the \"@\". and then split those texts by '.' \n",
        "after that remove the words whose length is less than or equal to 2 and also remove'com' word and then combine those words by space. \n",
        "In one doc, if we have 2 or more mails, get all.\n",
        "<b>Eg:[test@dm1.d.com, test2@dm2.dm3.com]-->[dm1.d.com, dm3.dm4.com]-->[dm1,d,com,dm2,dm3,com]-->[dm1,dm2,dm3]-->\"dm1 dm2 dm3\" </b> \n",
        "append all those into one list/array. ( This will give length of 18828 sentences i.e one list for each of the document). \n",
        "Some sample output was shown below. \n",
        "\n",
        "> In the above sample document there are emails [jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu]\n",
        "\n",
        "preprocessing:\n",
        "[jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu] ==> [nyx cs du edu mimsy umd edu cs umd edu] ==> \n",
        "[nyx edu mimsy umd edu umd edu]\n",
        "\n",
        "<font color='blue'><b>2.</b></font> Replace all the emails by space in the original text. \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KavKDD9FYv0p",
        "outputId": "0b87ab7b-46df-4995-eaca-4f5831ad223e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['juliet caltech edu',\n",
              "       'coding bchs edu newsgate sps mot austlcm sps mot austlcm sps mot com  dna bchs edu',\n",
              "       'batman bmd trw', ..., 'rbdc wsnc org dscomsa desy zeus  desy',\n",
              "       'rbdc wsnc org morrow stanford edu pangea Stanford EDU',\n",
              "       'rbdc wsnc org apollo apollo'], dtype=object)"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we have collected all emails and preprocessed them, this is sample output\n",
        "preprocessed_email"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obReqs55Yv0v",
        "outputId": "10770414-9be0-4d63-9587-5363a8c10c4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18828"
            ]
          },
          "execution_count": 29,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(preprocessed_email)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIovFDQzYv03"
      },
      "source": [
        "<pre>\n",
        "<font color='blue'><b>3.</b></font> Get subject of the text i.e. get the total lines where \"Subject:\" occur and remove \n",
        "the word which are before the \":\" remove the newlines, tabs, punctuations, any special chars.\n",
        "<b>Eg: if we have sentance like \"Subject: Re: Gospel Dating @ \\r\\r\\n\" --> You have to get \"Gospel Dating\"</b> \n",
        "Save all this data into another list/array. \n",
        "\n",
        "<font color='blue'><b>4.</b></font> After you store it in the list, Replace those sentances in original text by space.\n",
        "\n",
        "<font color='blue'><b>5.</b></font> Delete all the sentances where sentence starts with <b>\"Write to:\"</b> or <b>\"From:\"</b>.\n",
        "> In the above sample document check the 2nd line, we should remove that\n",
        "\n",
        "<font color='blue'><b>6.</b></font> Delete all the tags like \"< anyword >\"\n",
        "> In the above sample document check the 4nd line, we should remove that \"< 65882@mimsy.umd.edu >\"\n",
        "\n",
        "\n",
        "<font color='blue'><b>7.</b></font> Delete all the data which are present in the brackets. \n",
        "In many text data, we observed that, they maintained the explanation of sentence \n",
        "or translation of sentence to another language in brackets so remove all those.\n",
        "<b>Eg: \"AAIC-The course that gets you HIRED(AAIC - Der Kurs, der Sie anstellt)\" --> \"AAIC-The course that gets you HIRED\"</b>\n",
        "\n",
        "> In the above sample document check the 4nd line, we should remove that \"(Charley Wingate)\"\n",
        "\n",
        "\n",
        "<font color='blue'><b>8.</b></font> Remove all the newlines('\\n'), tabs('\\t'), \"-\", \"\\\".\n",
        "\n",
        "<font color='blue'><b>9.</b></font> Remove all the words which ends with <b>\":\"</b>.\n",
        "<b>Eg: \"Anyword:\"</b>\n",
        "> In the above sample document check the 4nd line, we should remove that \"writes:\"\n",
        "\n",
        "\n",
        "<font color='blue'><b>10.</b></font> Decontractions, replace words like below to full words. \n",
        "please check the donors choose preprocessing for this \n",
        "<b>Eg: can't -> can not, 's -> is, i've -> i have, i'm -> i am, you're -> you are, i'll --> i will </b>\n",
        "\n",
        "<b> There is no order to do point 6 to 10. but you have to get final output correctly</b>\n",
        "\n",
        "<font color='blue'><b>11.</b></font> Do chunking on the text you have after above preprocessing. \n",
        "Text chunking, also referred to as shallow parsing, is a task that \n",
        "follows Part-Of-Speech Tagging and that adds more structure to the sentence.\n",
        "So it combines the some phrases, named entities into single word.\n",
        "So after that combine all those phrases/named entities by separating <b>\"_\"</b>. \n",
        "And remove the phrases/named entities if that is a \"Person\". \n",
        "You can use <b>nltk.ne_chunk</b> to get these. \n",
        "Below we have given one example. please go through it. \n",
        "\n",
        "useful links: \n",
        "<a href='https://www.nltk.org/book/ch07.html'>https://www.nltk.org/book/ch07.html</a>\n",
        "<a href='https://stackoverflow.com/a/31837224/4084039'>https://stackoverflow.com/a/31837224/4084039</a>\n",
        "<a href='http://www.nltk.org/howto/tree.html'>http://www.nltk.org/howto/tree.html</a>\n",
        "<a href='https://stackoverflow.com/a/44294377/4084039'>https://stackoverflow.com/a/44294377/4084039</a>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "2lAaKQ6EYv04",
        "outputId": "a81e717e-a0f9-4df8-b022-4ab4b148ac7f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3430b9a6ac86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#i am living in the New York\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i am living in the New York -->\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
          ]
        }
      ],
      "source": [
        "#i am living in the New York\n",
        "print(\"i am living in the New York -->\", list(chunks))\n",
        "print(\" \")\n",
        "print(\"-\"*50)\n",
        "print(\" \")\n",
        "#My name is Srikanth Varma\n",
        "print(\"My name is Srikanth Varma -->\", list(chunks1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geKUZr_3lsx3"
      },
      "outputs": [],
      "source": [
        "import nltk,re,pprint\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVjBlPMak1Zh",
        "outputId": "a552fb7f-bd22-452a-ac7f-69c386392d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[('Welcome', 'VB'), ('to', 'TO'), ('the', 'DT'), ('true', 'JJ'), ('world', 'NN'), ('.', '.')], [('We', 'PRP'), ('are', 'VBP'), ('cool', 'JJ')]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def ie_preprocess(document):\n",
        "    sentences = nltk.sent_tokenize(document) \n",
        "    sentences = [nltk.word_tokenize(sent) for sent in sentences] \n",
        "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
        "    return sentences \n",
        "\n",
        "sentence = ie_preprocess(\"Welcome to the true world. We are cool\")\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmloyWOwpKHe",
        "outputId": "1bb09e85-02a0-4808-fdc6-286e2d4b390f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S Welcome/VB to/TO (NP the/DT true/JJ world/NN))\n"
          ]
        }
      ],
      "source": [
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
        "cp = nltk.RegexpParser(grammar) \n",
        "result = cp.parse(sentence[0]) \n",
        "print(result) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV8gzLUjYv0-"
      },
      "source": [
        "<pre>We did chunking for above two lines and then We got one list where each word is mapped to a \n",
        "POS(parts of speech) and also if you see \"New York\" and \"Srikanth Varma\", \n",
        "they got combined and represented as a tree and \"New York\" was referred as \"GPE\" and \"Srikanth Varma\" was referred as \"PERSON\". \n",
        "so now you have to Combine the \"New York\" with <b>\"_\"</b> i.e \"New_York\"\n",
        "and remove the \"Srikanth Varma\" from the above sentence because it is a person.</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpaC-KF3Yv1A"
      },
      "source": [
        "<pre>\n",
        "<font color='blue'><b>13.</b></font> Replace all the digits with space i.e delete all the digits. \n",
        "> In the above sample document, the 6th line have digit 100, so we have to remove that.\n",
        "\n",
        "<font color='blue'><b>14.</b></font> After doing above points, we observed there might be few word's like\n",
        " <b> \"_word_\" (i.e starting and ending with the _), \"_word\" (i.e starting with the _),\n",
        "  \"word_\" (i.e ending with the _)</b> remove the <b>_</b> from these type of words. \n",
        "\n",
        "<font color='blue'><b>15.</b></font>  We also observed some words like <b> \"OneLetter_word\"- eg: d_berlin, \n",
        "\"TwoLetters_word\" - eg: dr_berlin </b>, in these words we remove the \"OneLetter_\" (d_berlin ==> berlin) and \n",
        "\"TwoLetters_\" (de_berlin ==> berlin). i.e remove the words \n",
        "which are length less than or equal to 2 after spliiting those words by \"_\". \n",
        "\n",
        "<font color='blue'><b>16.</b></font> Convert all the words into lower case and lowe case \n",
        "and remove the words which are greater than or equal to 15 or less than or equal to 2.\n",
        "\n",
        "<font color='blue'><b>17.</b></font> replace all the words except \"A-Za-z_\" with space. \n",
        "\n",
        "<font color='blue'><b>18.</b></font> Now You got Preprocessed Text, email, subject. create a dataframe with those. \n",
        "Below are the columns of the df. \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB43OGEfYv1C",
        "outputId": "945bc8a4-1f99-4410-94c8-c776a405b5f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['text', 'class', 'preprocessed_text', 'preprocessed_subject',\n",
            "       'preprocessed_emails'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM6A19xFYv1I",
        "outputId": "9de13fa8-6604-49a2-8013-6b22f0a256a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text                    From: arc1@ukc.ac.uk (Tony Curtis)\\r\\r\\r\\nSubj...\n",
            "class                                                         alt.atheism\n",
            "preprocessed_text       said re is article if followed the quoting rig...\n",
            "preprocessed_subject                                christian morality is\n",
            "preprocessed_emails                                   ukc mac macalstr edu\n",
            "Name: 567, dtype: object\n"
          ]
        }
      ],
      "source": [
        "data.iloc[400]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfWUeIN1Yv1N"
      },
      "source": [
        "### To get above mentioned data frame --> Try to Write Total Preprocessing steps in One Function Named Preprocess as below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEGEHTNQYv1N"
      },
      "outputs": [],
      "source": [
        "def preprocess(Input_Text):\n",
        "    \"\"\"Do all the Preprocessing as shown above and\n",
        "    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data\"\"\"\n",
        "    return (list_of_preproessed_emails,subject,text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceASjKizYv1U"
      },
      "source": [
        "### Code checking:\n",
        "\n",
        "<font color='red' size=4>\n",
        "After Writing preprocess function. call that functoin with the input text of 'alt.atheism_49960' doc and print the output of the preprocess function\n",
        "<br>\n",
        "This will help us to evaluate faster, based on the output we can suggest you if there are any changes.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x3og_iaYv1S"
      },
      "source": [
        "### After writing Preprocess function, call the function for each of the document(18828 docs) and then create a dataframe as mentioned above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3ucJLtWYv1V"
      },
      "source": [
        "### Training The models to Classify: \n",
        "\n",
        "<pre>\n",
        "1. Combine \"preprocessed_text\", \"preprocessed_subject\", \"preprocessed_emails\" into one column. use that column to model. \n",
        "\n",
        "2. Now Split the data into Train and test. use 25% for test also do a stratify split. \n",
        "\n",
        "3. Analyze your text data and pad the sequnce if required. \n",
        "Sequnce length is not restricted, you can use anything of your choice. \n",
        "you need to give the reasoning\n",
        "\n",
        "4. Do Tokenizer i.e convert text into numbers. please be careful while doing it. \n",
        "if you are using tf.keras \"Tokenizer\" API, it removes the <b>\"_\"</b>, but we need that.\n",
        "\n",
        "5. code the model's ( Model-1, Model-2 ) as discussed below \n",
        "and try to optimize that models.  \n",
        "\n",
        "6. For every model use predefined Glove vectors. \n",
        "<b>Don't train any word vectors while Training the model.</b>\n",
        "\n",
        "7. Use \"categorical_crossentropy\" as Loss. \n",
        "\n",
        "8. Use <b>Accuracy and Micro Avgeraged F1 score</b> as your as Key metrics to evaluate your model. \n",
        "\n",
        "9.  Use Tensorboard to plot the loss and Metrics based on the epoches.\n",
        "\n",
        "10. Please save your best model weights in to <b>'best_model_L.h5' ( L = 1 or 2 )</b>. \n",
        "\n",
        "11. You are free to choose any Activation function, learning rate, optimizer.\n",
        "But have to use the same architecture which we are giving below.\n",
        "\n",
        "12. You can add some layer to our architecture but you <b>deletion</b> of layer is not acceptable.\n",
        "\n",
        "13. Try to use <b>Early Stopping</b> technique or any of the callback techniques that you did in the previous assignments.\n",
        "\n",
        "14. For Every model save your model to image ( Plot the model) with shapes \n",
        "and inlcude those images in the notebook markdown cell, \n",
        "upload those imgages to Classroom. You can use \"plot_model\" \n",
        "please refer <a href='https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model'>this</a> if you don't know how to plot the model with shapes. \n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0mwdtcvYv1X"
      },
      "source": [
        "### Model-1: Using 1D convolutions with word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXPPsovJ3ePk"
      },
      "source": [
        "<pre>\n",
        "<b>Encoding of the Text </b> --> For a given text data create a Matrix with Embedding layer as shown Below. \n",
        "In the example we have considered d = 5, but in this assignment we will get d = dimension of Word vectors we are using.\n",
        " i.e if we have maximum of 350 words in a sentence and embedding of 300 dim word vector, \n",
        " we result in 350*300 dimensional matrix for each sentance as output after embedding layer\n",
        "<img src='https://i.imgur.com/kiVQuk1.png'>\n",
        "Ref: https://i.imgur.com/kiVQuk1.png\n",
        "\n",
        "<b>Reference:</b>\n",
        "<a href='https://stackoverflow.com/a/43399308/4084039'>https://stackoverflow.com/a/43399308/4084039</a>\n",
        "<a href='https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/'>https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/</a>\n",
        "\n",
        "<b><a href='https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work'>How EMBEDDING LAYER WORKS </a></b>\n",
        "\n",
        "</pre>\n",
        "\n",
        "### Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGVQKge3Yv1e"
      },
      "source": [
        "<img src='https://i.imgur.com/fv1GvFJ.png'>\n",
        "ref: 'https://i.imgur.com/fv1GvFJ.png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6SBG5AYv1f"
      },
      "source": [
        "<pre>\n",
        "1. all are Conv1D layers with any number of filter and filter sizes, there is no restriction on this.\n",
        "\n",
        "2. use concatenate layer is to concatenate all the filters/channels. \n",
        "\n",
        "3. You can use any pool size and stride for maxpooling layer.\n",
        "\n",
        "4. Don't use more than 16 filters in one Conv layer becuase it will increase the no of params. \n",
        "( Only recommendation if you have less computing power )\n",
        "\n",
        "5. You can use any number of layers after the Flatten Layer.\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cg4L1V4Yv1d"
      },
      "source": [
        "### Model-2 : Using 1D convolutions with character embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Djg4YVA3oQx"
      },
      "source": [
        "<pre>\n",
        "<pre><img src=\"https://i.ytimg.com/vi/CNY8VjJt-iQ/maxresdefault.jpg\" width=\"70%\">\n",
        "Here are the some papers based on Char-CNN\n",
        " 1. Xiang Zhang, Junbo Zhao, Yann LeCun. <a href=\"http://arxiv.org/abs/1509.01626\">Character-level Convolutional Networks for Text Classification</a>.NIPS 2015\n",
        " 2. Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush. <a href=\"https://arxiv.org/abs/1508.06615\">Character-Aware Neural Language Models</a>. AAAI 2016\n",
        " 3. Shaojie Bai, J. Zico Kolter, Vladlen Koltun. <a href=\"https://arxiv.org/pdf/1803.01271.pdf\">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</a>\n",
        " 4. Use the pratrained char embeddings <a href='https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt'>https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt</a>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXvKSEIeSvN5"
      },
      "source": [
        "<img src='https://i.imgur.com/EuuoJtr.png'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLmNSgC3pVkC"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "\n",
        "with codecs.open(\"/content/documents/alt.atheism_49960.txt\",\"r\",encoding=\"utf-8\") as sourceFile:\n",
        "    with codecs.open(\"Your UTF-8 output file.txt\",\"w\",encoding=\"UTF-8\") as targetFile:\n",
        "        while True:\n",
        "            contents = sourceFile.read()\n",
        "            if not contents:\n",
        "                break\n",
        "            targetFile.write(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuwutYW4OHvR",
        "outputId": "1ce09d2f-68af-4a8a-dd46-7033fe5a9bc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'\\n  i am\\\\checking -if every\\n  tab is detected  or not anyone:\\n\\n\\n'\n",
            "'  i amchecking if every  tab is detected  or not '\n"
          ]
        }
      ],
      "source": [
        "text=\"\"\"\n",
        "  i am\\checking -if every\n",
        "  tab is detected  or not anyone:\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "print(repr(text))\n",
        "text = re.sub(r\"[\\n|\\t|\\\\|-]\",\"\", text)\n",
        "text = re.sub(r\"\\w+:\",\"\",text)\n",
        "print(repr(text))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Doc_classification(CNN)_data_processing_and_split.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}