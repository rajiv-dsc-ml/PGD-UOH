{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bootstrap_assignment_sol_sub.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sNKZq4XrXQh"
      },
      "source": [
        "# <font color='red'><b>Bootstrap assignment</b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAHap1Z3FZC-"
      },
      "source": [
        "<b>There will be some functions that start with the word \"grader\" ex: grader_sampples(), grader_30().. etc, you should not change those function definition.\n",
        "\n",
        "Every Grader function has to return True.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuxBq_bvrwh2"
      },
      "source": [
        "<font color='blue'> <b>Importing packages</b> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6ag91ijrQOs"
      },
      "source": [
        "import numpy as np # importing numpy for numerical computation\n",
        "from sklearn.datasets import load_boston # here we are using sklearn's boston dataset\n",
        "from sklearn.metrics import mean_squared_error # importing mean_squared_error metric\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfAtYoETUQmX"
      },
      "source": [
        "import pandas as pd # since load_boston was not working('ethical issue') . I needed pandas to read the csv downloaded from oiginal source\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-NTDR8WJBDY"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcHOsONTt1K_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7faf5be1-bfca-41ac-ee70-3642e4c68425"
      },
      "source": [
        "#It was not working. The reason quoted \"ethical\"\n",
        "\"\"\"boston = load_boston()\n",
        "x=boston.data #independent variables\n",
        "y=boston.target #target variable\"\"\""
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'boston = load_boston()\\nx=boston.data #independent variables\\ny=boston.target #target variable'"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG6v5OgkUFlv"
      },
      "source": [
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "x = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]) #independent variables\n",
        "y = raw_df.values[1::2, 2] #dependent variables"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc1htEFYuLRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "246be43d-563d-41d1-ffd1-20799622d791"
      },
      "source": [
        "\n",
        "x.shape,y.shape"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((506, 13), (506,))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEa_HqRZloH4"
      },
      "source": [
        "## <font color='red'><b>Task 1</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ5q8IxHNRk3"
      },
      "source": [
        "<font color='red'> <b>Step - 1</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJCFCaOzl7Mr"
      },
      "source": [
        "*  <font color='blue'><b>Creating samples</b></font><br>\n",
        "    <b> Randomly create 30 samples from the whole boston data points</b>\n",
        "    *  Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points\n",
        "    \n",
        "     For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly , consider we have selected [4, 5, 7, 8, 9, 3] now we will replicate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]\n",
        "* <font color='blue'><b> Create 30 samples </b></font>\n",
        "    *  Note that as a part of the Bagging when you are taking the random samples <b>make sure each of the sample will have different set of columns</b><br>\n",
        "Ex: Assume we have 10 columns[1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10] for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample  [7, 9, 1, 4, 5, 6, 2] and so on...\n",
        "Make sure each sample will have atleast 3 feautres/columns/attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUqFEBSvNjCa"
      },
      "source": [
        "<font color='red'><b>Step - 2 </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqi9AhCYNq3Z"
      },
      "source": [
        "<font color='blue'><b>Building High Variance Models on each of the sample and finding train MSE value</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lLBnZHXOFln"
      },
      "source": [
        "*  Build a regression trees on each of 30 samples.\n",
        "*  Computed the predicted values of each data point(506 data points) in your corpus.\n",
        "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$\n",
        "*  Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kls23JLnSN23"
      },
      "source": [
        "<font color='red'> <b>Step - 3 </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz2GchkGSWnh"
      },
      "source": [
        "*  <font color='blue'><b>Calculating the OOB score </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGHkVV2kSibm"
      },
      "source": [
        "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.\n",
        "*  Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK860ocxTyoz"
      },
      "source": [
        "# <font color='red'><b>Task 2</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dme-N6TUCrY"
      },
      "source": [
        "*  <font color='blue'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
        "  *   Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
        "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
        "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
        "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
        "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6UcH1x9Uwrj"
      },
      "source": [
        "# <font color='red'><b>Task 3</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOC_AgsLU7OH"
      },
      "source": [
        "*  <font color='blue'><b>Given a single query point predict the price of house.</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYs5jSFdVILe"
      },
      "source": [
        "Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] \n",
        "Predict the house price for this point as mentioned in the step 2 of Task 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6gxZEsFWm-8"
      },
      "source": [
        "<br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2fHTdS_zpgG"
      },
      "source": [
        "# <font color='blue'> <b>Task - 1</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0yGBuryOwHz"
      },
      "source": [
        "<font color='blue'><b>Step - 1</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJXX8vf3z073"
      },
      "source": [
        "*  <font color='blue'> <b>Creating samples</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSVaWG1F4uCZ"
      },
      "source": [
        "<font color='Orange'><b>Algorithm</b></font>\n",
        "\n",
        "![alt text](https://i.imgur.com/BTVYXQ1.jpg/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_oWoN97BhDY"
      },
      "source": [
        "*  <font color='blue'><b> Write code for generating samples</b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph_6D2SDzz7F"
      },
      "source": [
        "def generating_samples(input_data, target_data):\n",
        "\n",
        "    '''In this function, we will write code for generating 30 samples '''\n",
        "    # you can use random.choice to generate random indices without replacement\n",
        "    # Please have a look at this link https://docs.scipy.org/doc/numpy-1.16.0/reference/generated/numpy.random.choice.html for more details\n",
        "    # Please follow above pseudo code for generating samples \n",
        "    \n",
        "    #for i in range(30):\n",
        "    #rand_size is to decide how much columns to sample, ie \"how many columns\"\n",
        "    #rand_sizes is to decide which columns will be sampled , i.e \"which are those columns\"\n",
        "    rand_size = np.random.randint(low= 3,high= 14,size=1) #as the range is [low,high), and we want min_size=3 and max_size=13\n",
        "    rand_indices = np.random.choice(a = 13, size=rand_size, replace=False) #we want array like [1,2,3] or[2,6,3,8] #a =integer is treated as np.arange(integ)\n",
        "    row_indices = np.random.choice(a=506, size= 303, replace= False) #\"which will be those rows?\"\n",
        "    \n",
        "    sample = input_data[row_indices,:][:,rand_indices]\n",
        "    y_sample = target_data[row_indices]\n",
        "\n",
        "    psuedo_ind = np.random.choice(a=303,size=203,replace=True)\n",
        "\n",
        "    psuedo_sample = sample[psuedo_ind,:] #as sample has already some columns and psuedo sample is part of it\n",
        "    psuedo_y = y_sample[psuedo_ind] #psuedo_y is part of y_sample corresponding to pseudo sample\n",
        "\n",
        "    final_sam = np.concatenate((sample,psuedo_sample),axis=0) # this is concatenation part, so that finally, we get 506 data points\n",
        "    final_y =np.concatenate((y_sample,psuedo_y)) #this is same concatenation part for target variable.\n",
        "\n",
        "    #samples_lst.append(final_sam)\n",
        "    #y_lst.append(final_y)\n",
        "\n",
        "    return final_sam,final_y,row_indices , rand_indices\n",
        "    \n",
        "\n",
        "    # return sampled_input_data , sampled_target_data,selected_rows,selected_columns\n",
        "    #note please return as lists"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MivEQFlm7iOg"
      },
      "source": [
        "<font color='cyan'> <b> Grader function - 1 </b> </fongt>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVvuhNzm7uld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9790a3c0-588a-4a36-b249-3d7602949f11"
      },
      "source": [
        "def grader_samples(a,b,c,d):\n",
        "    length = (len(a)==506  and len(b)==506)\n",
        "    sampled = (len(a)-len(set([str(i) for i in a]))==203)\n",
        "    rows_length = (len(c)==303)\n",
        "    column_length= (len(d)>=3)\n",
        "    assert(length and sampled and rows_length and column_length)\n",
        "    return True\n",
        "a,b,c,d = generating_samples(x, y)\n",
        "grader_samples(a,b,c,d)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4LSsmn4Jn2_"
      },
      "source": [
        "*  <font color='blue'> <b>Create 30 samples </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ec7MN6sL2BZ"
      },
      "source": [
        "![alt text](https://i.imgur.com/p8eZaWL.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXlKWjCcBvTk"
      },
      "source": [
        "# Use generating_samples function to create 30 samples \n",
        "# store these created samples in a list\n",
        "def create_30sample(x,y):\n",
        "  list_input_data =[]\n",
        "  list_output_data =[]\n",
        "  list_selected_row= []\n",
        "  list_selected_columns=[]\n",
        "\n",
        "  for i in range(30):\n",
        "    a,b,c,d = generating_samples(x, y)\n",
        "    list_input_data.append(a)\n",
        "    list_output_data.append(b)\n",
        "    list_selected_row.append(c)\n",
        "    list_selected_columns.append(d)\n",
        "  \n",
        "  return list_input_data, list_output_data, list_selected_row, list_selected_columns\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I90c9ocgVnCf"
      },
      "source": [
        "list_input_data, list_output_data, list_selected_row, list_selected_columns = create_30sample(x,y)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXUz9VFiMQkh"
      },
      "source": [
        "<font color='cyan'> <b>Grader function - 2 </b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCvIq8NuMWOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ba29bc-620d-42fc-9f45-e6dff14b4314"
      },
      "source": [
        "def grader_30(a):\n",
        "    assert(len(a)==30 and len(a[0])==506)\n",
        "    return True\n",
        "grader_30(list_input_data)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pv-mkZkO6dh"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whaHCPB0O8qF"
      },
      "source": [
        "<font color='red'><b>Step - 2 </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBy4zXSWPtU8"
      },
      "source": [
        "<font color='orange'><b>Flowchart for building tree</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xvH06HPQBdP"
      },
      "source": [
        "![alt text](https://i.imgur.com/pcXfSmp.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRwPO_uHQjul"
      },
      "source": [
        "*  <font color='blue'><b> Write code for building regression trees</b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWQp6tRwMthq"
      },
      "source": [
        "def modeling(list_input_data, list_output_data, list_selected_row, list_selected_columns):\n",
        "  base_models = []\n",
        "  preds = []\n",
        "  for i in range(30):\n",
        "    X=list_input_data[i] #crazy_x(506 points)\n",
        "    Y = list_output_data[i] #crazy_y (len =506)\n",
        "    tree_reg = DecisionTreeRegressor()\n",
        "    tree_reg.fit(X,Y) #(status:debugged) earlier, y = list_output_data[i] , was used in two lines before and in this line,\n",
        "                      # tree_reg.fit(X,y) was written, small y is a global variable(storing 506 ground truth labels) and may be this line took that  y to train. The difference was too noticeable , reducing mse from 146 to 2.6\n",
        "                      #it also led me to doubt \"local shadows gloabal\" maxim\n",
        "    base_models.append(tree_reg)\n",
        "    \n",
        "  for i in range(30):\n",
        "    model = base_models[i]\n",
        "    cols = list_selected_columns[i]\n",
        "    pred = model.predict(x[:,cols]) #one set of preds of 506 (506,)actual pts by a base_model\n",
        "    preds.append(pred)\n",
        "\n",
        "  final_preds = np.sum(preds,axis=0)/30\n",
        "\n",
        "  mse = (( final_preds - y )**2).mean()\n",
        "\n",
        "  preds_oob = []\n",
        "  count_nan = 0\n",
        "  for i in range(506):\n",
        "    x_row = x[i] #x was a global variable . So, when the function is called, it can access 'x' which is original dataset(excluding target var) storing variable\n",
        "    x_row_preds = []\n",
        "    for j in range(30):\n",
        "      row = list_selected_row[j]#see the jth sample of 506 pts, among all 30 samples\n",
        "      if i in row:              #this jth sample contains 506 indices of corresponding x's. whether index i present in this jth sample?\n",
        "        pass\n",
        "      else:\n",
        "        my_base = base_models[j] #every jth sample of 506 pts refer to 506 points of x, which have been trained to the jth model\n",
        "        col_ar = list_selected_columns[j]#likewise, jth columns indices show that the jth sample of x's had those columns\n",
        "        my_pred = my_base.predict([x_row[col_ar]])\n",
        "        x_row_preds.append(my_pred)\n",
        "        \n",
        "    #print(x_row_preds)\n",
        "    if len(x_row_preds) == 0: #if this x datapoint happens to be seen by each base model and so couldn't be used as oob's element\n",
        "      preds_oob.append(y[i])#y[i])  #we input None as the oob predn of this point in the preds_oob list which contains corresponding oob pdns of 506 data pts\n",
        "      #entring None will create problem at the time of clac (pred_oob-y). So, will give the exact value of y[i] in oob_pred of ith point , so that the diff\n",
        "      #will be zero in calc (pred_oob - y) and we will keep count of such x's and adjust the number of instances for oob_preds\n",
        "      count_nan += 1\n",
        "      print(count_nan)\n",
        "    else:\n",
        "      x_rowmean = sum(x_row_preds)/len(x_row_preds)\n",
        "      preds_oob.append(x_rowmean)\n",
        "\n",
        "  preds_oob = np.array(preds_oob)\n",
        "  sq_diff = (y - preds_oob.flatten())**2 #.flatten() is a very important step. Note that the DT expects a 2d array as input, so for each x the DT gives output in 1d array form. and preds_oob becomes a 2d array which when subracted from y(1d array) will give unintentional result\n",
        "  oob_score = sq_diff.sum()/(len(sq_diff) - count_nan)\n",
        "\n",
        "\n",
        "  return mse,oob_score,base_models"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21j8BKfAQ1U8"
      },
      "source": [
        "<font color='orange'><b>Flowchart for calculating MSE </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q0mTBD2RBx_"
      },
      "source": [
        "![alt text](https://i.imgur.com/sPEE618.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e-UamlHRjPy"
      },
      "source": [
        "After getting predicted_y for each data point, we can use sklearns mean_squared_error to calculate the MSE between predicted_y and actual_y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnIMT7_oR312"
      },
      "source": [
        "*  <font color='blue'><b> Write code for calculating MSE</b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWhcvMRWRA9b"
      },
      "source": [
        "#Please refer to  \"Write code for building regression trees\" block above\n",
        "#I have implemented the logic for mse under the function named, 'modeling'\n",
        "def mse(final_preds):\n",
        "  mse = (( final_preds - y )**2).mean()\n",
        "  return mse"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuclPDMnSz8F"
      },
      "source": [
        "<font color='blue'><b>Step - 3 </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESb9FSIDTM5V"
      },
      "source": [
        "<font color='orange'><b>Flowchart for calculating OOB score</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB-d6NMETbd9"
      },
      "source": [
        "![alt text](https://i.imgur.com/95S5Mtm.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW3GOcFzTqbt"
      },
      "source": [
        "Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBqcS03pUYSZ"
      },
      "source": [
        "*  <font color='blue'><b> Write code for calculating OOB score </b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayRI7EXRN5r6"
      },
      "source": [
        "#Please refer to  \"Write code for building regression trees\" block above\n",
        "#I have implemented the logic for oob-score under the function named, 'modeling'\n",
        "\n",
        "\n",
        "def cal_oob_score():\n",
        "  preds_oob = []\n",
        "  count_nan = 0\n",
        "  for i in range(506):\n",
        "    x_row = x[i] #x was a global variable . So, when the function is called, it can access 'x' which is original dataset(excluding target var) storing variable\n",
        "    x_row_preds = []\n",
        "    for j in range(30):\n",
        "      row = list_selected_row[j]#see the jth sample of 506 pts, among all 30 samples\n",
        "      if i in row:              #this jth sample contains 506 indices of corresponding x's. whether index i present in this jth sample?\n",
        "        pass\n",
        "      else:\n",
        "        my_base = base_models[j] #every jth sample of 506 pts refer to 506 points of x, which have been trained to the jth model\n",
        "        col_ar = list_selected_columns[j]#likewise, jth columns indices show that the jth sample of x's had those columns\n",
        "        my_pred = my_base.predict([x_row[col_ar]])\n",
        "        x_row_preds.append(my_pred)\n",
        "        \n",
        "    #print(x_row_preds)\n",
        "    if len(x_row_preds) == 0: #if this x datapoint happens to be seen by each base model and so couldn't be used as oob's element\n",
        "      preds_oob.append(y[i])#y[i])  #we input None as the oob predn of this point in the preds_oob list which contains corresponding oob pdns of 506 data pts\n",
        "      #entring None will create problem at the time of clac (pred_oob-y). So, will give the exact value of y[i] in oob_pred of ith point , so that the diff\n",
        "      #will be zero in calc (pred_oob - y) and we will keep count of such x's and adjust the number of instances for oob_preds\n",
        "      count_nan += 1\n",
        "    else:\n",
        "      x_rowmean = sum(x_row_preds)/len(x_row_preds)\n",
        "      preds_oob.append(x_rowmean)\n",
        "\n",
        "  preds_oob = np.array(preds_oob)\n",
        "  sq_diff = (y - preds_oob.flatten())**2 #.flatten() is a very important step. Note that the DT expects a 2d array as input, so for each x the DT gives output in 1d array form. and preds_oob becomes a 2d array which when subracted from y(1d array) will give unintentional result\n",
        "  oob_score = sq_diff.sum()/(len(sq_diff) - count_nan)\n",
        "  return oob_score"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbuiwX3OUjUI"
      },
      "source": [
        "# <font color='blue'><b>Task 2</b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceW5-D88Uswi"
      },
      "source": [
        "mse_lst = []\n",
        "oob_score_lst = []\n",
        "for k in range(35):\n",
        "  list_input_data, list_output_data, list_selected_row, list_selected_columns = create_30sample(x,y)\n",
        "  mse, oob_score, base_models = modeling(list_input_data, list_output_data, list_selected_row, list_selected_columns)\n",
        "  mse_lst.append(mse)\n",
        "  oob_score_lst.append(oob_score)\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69YgP0DjKSu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e669f9-820c-4f23-d53e-d5bda3d9dfc2"
      },
      "source": [
        "mse_ar = np.array(mse_lst)\n",
        "oob_score_ar = np.array(oob_score_lst)\n",
        "\n",
        "est_m = mse_ar.mean()\n",
        "band_m = mse_ar.std()/np.sqrt(len(mse_ar))\n",
        "\n",
        "mse_conf_int = (est_m - 1.98*band_m, est_m+ 1.98*band_m)\n",
        "\n",
        "est_o = oob_score_ar.mean()\n",
        "band_o = oob_score_ar.std()/np.sqrt(len(oob_score_ar))\n",
        "\n",
        "oob_conf_int = (est_o - 1.98*band_o, est_o + 1.98*band_o)\n",
        "\n",
        "print(mse_conf_int)\n",
        "print(oob_conf_int)\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2.221769480299384, 2.406179563057043)\n",
            "(13.059809097528671, 13.852805285761747)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKTnJdiBVS_e"
      },
      "source": [
        "# <font color='blue'><b>Task 3</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXxrvZqHV1Fr"
      },
      "source": [
        "<font color='orange'><b>Flowchart for Task 3</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyjwEJ62V6a6"
      },
      "source": [
        "<b>Hint: </b> We created 30 models by using 30 samples in TASK-1. Here, we need send query point \"xq\"  to 30 models and perform the regression on the output generated by 30 models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0emSwLL7VurD"
      },
      "source": [
        "![alt text](https://i.imgur.com/Y5cNhQk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29hjwKlWWDfo"
      },
      "source": [
        "*  <font color='blue'><b> Write code for TASK 3 </b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_pUlSD-VYD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a433fb-e366-48a8-bf2a-abab8956da45"
      },
      "source": [
        "xq= np.array([0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60])\n",
        "\n",
        "list_input_data, list_output_data, list_selected_row, list_selected_columns = create_30sample(x,y)\n",
        "mse, oob_score, base_models = modeling(list_input_data, list_output_data, list_selected_row, list_selected_columns)#this should be taken from above\n",
        "preds = []\n",
        "\n",
        "for i in range(len(base_models)):\n",
        "    model = base_models[i]\n",
        "    cols = list_selected_columns[i]\n",
        "    pred = model.predict([xq[cols]]) #one set of preds of 506 (506,)actual pts by a base_model\n",
        "    preds.append(pred)\n",
        "preds = np.array(preds)\n",
        "final_pred = np.sum(preds.flatten(),axis=0)/30\n",
        "print(final_pred)\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19.783333333333328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXkPHrpgTQ8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6143f841-779f-4d2c-8cf3-e0ef9d65f3ee"
      },
      "source": [
        "xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60]\n",
        "xqa = np.array(xq)\n",
        "print(xqa[[1,2,3]])\n",
        "xqa[[1,2,3]]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20.  5.  0.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([20.,  5.,  0.])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJHTGEZgWJjR"
      },
      "source": [
        "<br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOdUi-0xWOJ9"
      },
      "source": [
        "<font color='red'><b>Write observations for task 1, task 2, task 3 indetail</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview:\n",
        "Wisdom of crowd: Ensemble Models\n",
        "\n",
        "\"\"\"\n",
        "Suppose you pose a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this \n",
        "aggregated answer is better than an expert's answer.\n",
        "This is called wisdom of crowd\n",
        "\"\"\"\n",
        "---O'Reily book\n",
        "\n",
        "\"\"\"\n",
        "In fact, even if each classifier is a weak learner(only slightly better than random guessing), the ensemble can still be a strong \n",
        "learner, provided\n",
        "there are a sufficient number of weak learners and they are sufficiently diverse.\n",
        "\"\"\"\n",
        "---O'Reily book\n",
        "\n",
        "#Task1: This is task of Bagging \n",
        "        One way to get a diverse set of classifiers is to use very different training algorithms\n",
        "        Another way is to use the same training algo for every predictor(here we have used Decision Tree) and train them on different \n",
        "        random subsets of the training set\n",
        "        Bagging is to sample these random subbsets where replacement is allowed.\n",
        "\n",
        "        Note that this idea can be applied on columns also. For columns, we sample random subsets fromm set of all the columns without \n",
        "        replacement\n",
        "        This is called Column Pasting\n",
        "\n",
        "        I have tried to explain my way of implementation through comments alongside codes\n",
        "        I noticed that each model had to be trained on 506 points.\n",
        "        And so, to ensure that each model encounter diverse 506 points, we firstly sampled 60% points(without repeatition) and \n",
        "        the rest 40% points were sample with replacement from the first subset of 60% points\n",
        "\n",
        "        Then we keep lists to keep track of the information required about dataset we would give to each base model(total 30 such and \n",
        "                                                                                                                    hence 30 samples);\n",
        "        \"list_input_data, list_output_data, list_selected_row, list_selected_columns\"\n",
        "\n",
        "        Then in function named 'modeling', we give these info, and train each base-model on each distinct(most likely) \n",
        "        sample(result of row-bagging and column-pasting) \n",
        "        We store these trained base learners in  a list named 'base_models'\n",
        "        Each base learner would give different predictions on the original dataset\n",
        "        So, we have a list named preds, which stores prediction of each base learner. \n",
        "        So, basically,  for one query point, there are 30 predictions. So, we take average and report the value of prediction\n",
        "        So, final_prediction can be get by \"final_preds = np.sum(preds,axis=0)/30\"\n",
        "\n",
        "        Now, we can compare this set of final 503 prediction values with original target variables and calculate mse\n",
        "\n",
        "        For, calculation of  oob , we need to a bit vigilantive in observing the data points\n",
        "        For each data point , can we know how many models have not observed that point?\n",
        "        Yes, list_selected_row has information of all the rows each model was trained on.\n",
        "        So, for each point , we can check if a model was trained on it or not.\n",
        "        Example, \n",
        "        base_model[0] was trained on some points , which points? => the points whose indices are stored in list_selected_row[0]       \n",
        "        x[0] is a datapoint . To know , if this datapoint was seen by base_model[0], all we need to check if its index is present in \n",
        "        list_selected_row[0]\n",
        "\n",
        "        Using this logic, we calculate predicted value of each ddatapoint by all the base learners who were not trained on it.\n",
        "        The significance of such a score is that it tells us about generalization capacity of the model. This is what I thought.\n",
        "        \n",
        "\n",
        "#Task2: Basically, just throwing out aggregate MSE and aggregate OOB scores are too pithy way to  report the performance  of our \n",
        "        Random Forest model\n",
        "        Each time, we are experimenting, basically. Bagging rows and pasting columns, i.e juggling data randomly to create diverse \n",
        "        datasets, training base learners\n",
        "        on these diverese data sets and reporting measures, mse and oob_score\n",
        "        If such experimentation is prepeated , how much vaiability can we expect for such a statistic?\n",
        "        Why not to report a confidence interval at a particular level. Is it too wide? Is it narrow? \n",
        "        we can calculate mse and oob score for one iteration at a time. Each time, mse and oob-score neednot be same as their\n",
        "         precedents \n",
        "        in previous epochs. To get an idea of our Random Forest model's performance, we have repeated the experimenation 35 times. \n",
        "        In end, we have a list of 35 mses and 35 oob-scores\n",
        "        We borrow idea from Statistics and apply on these 35 observations to report a confidence interval.\n",
        "        \n",
        "\n",
        "#Task 3: It is just a simple prediction task performed by our Random Forest model of a given query point "
      ],
      "metadata": {
        "id": "AIcax45hWKT-"
      }
    }
  ]
}