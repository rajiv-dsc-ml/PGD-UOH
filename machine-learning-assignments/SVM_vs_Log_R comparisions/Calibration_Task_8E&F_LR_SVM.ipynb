{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Calibration Task:8E&F_LR_SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HExLQrE4ZxR"
      },
      "source": [
        "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LuKrFzC4ZxV"
      },
      "source": [
        "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wES-wWN4ZxX"
      },
      "source": [
        "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
        "\n",
        "Check the documentation for better understanding of these attributes: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
        "\n",
        "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
        "\n",
        "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
        "\n",
        "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
        "\n",
        "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
        "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
        "\n",
        "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
        "\n",
        "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z830CfMk4Zxa"
      },
      "source": [
        "## Task E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuBxHiCQ4Zxc"
      },
      "source": [
        "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
        "\n",
        "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
        "\n",
        "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCgMNEvI4Zxf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANUNIqCe4Zxn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "134da84a-a06c-48f1-c7a9-38c48e2155f8"
      },
      "source": [
        "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train, test = train_test_split(df, test_size=0.2)\n",
        "\n",
        "#np.vstack(X,y)\n",
        "#print(y.shape)\n",
        "y = np.reshape(y,(len(y),1))\n",
        "#print(y.shape)\n",
        "data=np.hstack((X,y))\n",
        "print(y.dtype)\n",
        "\n",
        "df = pd.DataFrame(data = data, columns=['f1','f2','f3','f4','f5','y'])\n",
        "\n",
        "print(df.dtypes)\n",
        "df['y'] = df['y'].apply(lambda y_ele: int(y_ele))\n",
        "print(df.dtypes)\n",
        "df['y'][df['y']==0]=-1\n",
        "df.head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int64\n",
            "f1    float64\n",
            "f2    float64\n",
            "f3    float64\n",
            "f4    float64\n",
            "f5    float64\n",
            "y     float64\n",
            "dtype: object\n",
            "f1    float64\n",
            "f2    float64\n",
            "f3    float64\n",
            "f4    float64\n",
            "f5    float64\n",
            "y       int64\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.353756</td>\n",
              "      <td>0.289293</td>\n",
              "      <td>0.485230</td>\n",
              "      <td>0.622031</td>\n",
              "      <td>-1.249362</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.204255</td>\n",
              "      <td>1.604311</td>\n",
              "      <td>-0.065753</td>\n",
              "      <td>-0.004749</td>\n",
              "      <td>1.548880</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.297654</td>\n",
              "      <td>-0.534487</td>\n",
              "      <td>0.236149</td>\n",
              "      <td>0.270045</td>\n",
              "      <td>-1.174866</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.361043</td>\n",
              "      <td>-1.127174</td>\n",
              "      <td>-0.485456</td>\n",
              "      <td>-0.662865</td>\n",
              "      <td>0.546729</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.305640</td>\n",
              "      <td>-0.838459</td>\n",
              "      <td>0.110520</td>\n",
              "      <td>0.097911</td>\n",
              "      <td>-1.043686</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1        f2        f3        f4        f5  y\n",
              "0  0.353756  0.289293  0.485230  0.622031 -1.249362 -1\n",
              "1  0.204255  1.604311 -0.065753 -0.004749  1.548880  1\n",
              "2 -1.297654 -0.534487  0.236149  0.270045 -1.174866 -1\n",
              "3 -0.361043 -1.127174 -0.485456 -0.662865  0.546729  1\n",
              "4 -1.305640 -0.838459  0.110520  0.097911 -1.043686 -1"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "3pE2l7dHk2Yu",
        "outputId": "5e2f9d4a-0e91-45b6-8686-b3930025f7c4"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.histplot(data = df, x = 'y')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7b79d13890>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "odkZzMMmp882",
        "outputId": "1c3bd8e4-eba0-424c-8996-12a35c15808b"
      },
      "source": [
        "df.groupby('y')['y'].count().reset_index(name ='count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>3486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1514</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   y  count\n",
              "0 -1   3486\n",
              "1  1   1514"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHie1zqH4Zxt"
      },
      "source": [
        "### Pseudo code\n",
        "\n",
        "clf = SVC(gamma=0.001, C=100.)<br>\n",
        "clf.fit(Xtrain, ytrain)\n",
        "\n",
        "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
        "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
        "    \n",
        "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
        "\n",
        "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h43kDT3M41u5"
      },
      "source": [
        "# you can write your code here\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2)\n",
        "train, cv = train_test_split(train, test_size = 0.25)\n",
        "\n",
        "assert(train.shape == (3000,6) and cv.shape == (1000,6) and test.shape == (1000,6))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co7pMAqa1niW"
      },
      "source": [
        "#Xtrain, ytrain, Xcv, ycv, Xtest, ytest\n",
        "Xtrain = train.drop(['y'], axis=1).values\n",
        "ytrain = train['y'].values\n",
        "\n",
        "Xcv = cv.drop(['y'], axis=1).values\n",
        "ycv = cv['y'].values\n",
        "\n",
        "Xtest = test.drop(['y'], axis=1).values\n",
        "ytest = test['y'].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYJlMwpb2W-M",
        "outputId": "2736d012-b542-40e2-c5e7-ee12eb6e3d90"
      },
      "source": [
        "Xtrain.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6qvQNJ_7eeV",
        "outputId": "1ccaff06-fefe-46e5-97e0-9197c01124c5"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "rbf_svm = Pipeline([\n",
        "                    (\"scalar\", StandardScaler()), (\"svm_clf\", SVC(gamma=0.001,C=100))\n",
        "])\n",
        "\n",
        "rbf_svm.fit(Xtrain, ytrain)\n",
        "print(\"train_error\", np.count_nonzero(rbf_svm.predict(Xtrain) - ytrain)*100/len(ytrain), \"%\")\n",
        "\n",
        "print(\"cv_error\", np.count_nonzero(rbf_svm.predict(Xcv) - ycv)*100/len(ycv), \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_error 7.3 %\n",
            "cv_error 8.1 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCLUkj8X8qw-"
      },
      "source": [
        " #rbf_svm.decision_function(Xcv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igI11Ugr6ZJV"
      },
      "source": [
        "rbf_svm.predict(Xtrain)\n",
        "#(∑all the support vectorsi=1(yiαiK(xi,xq))+intercept)\n",
        "#how to extract alpha'is\n",
        "##https://stackoverflow.com/questions/33860938/how-to-get-all-alpha-values-of-scikit-learn-svm-classifier\n",
        "##alphas = np.abs(svm.dual_coef_), but why?\n",
        "## svm.dual_coef_ is the product of the Lagrange multiplier alpha and the label of a data point. \n",
        "##and sklearn.model.SVC internally works with label -1 and 1\n",
        "\n",
        "\n",
        "#retrieving support vectors and dual coefficients\n",
        "dual_cof = rbf_svm[1].dual_coef_\n",
        "dual_cof=np.reshape(dual_cof,(dual_cof.shape[1],))\n",
        "sup_vectors = rbf_svm[1].support_vectors_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfuAPMOzpvPL"
      },
      "source": [
        "def rbf_similarity(x1, x2, gamma= 0.001):\n",
        "  dist=((x1-x2)**2).sum(axis=1)*0.001\n",
        "  #print(dist)\n",
        "  return np.exp(-dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeqFgbyI6AIq"
      },
      "source": [
        "cv_class = list()\n",
        "for ele in Xcv:\n",
        "  similarity_mat = rbf_similarity(sup_vectors, ele)\n",
        "  #print(X)\n",
        "  #print(similarity_mat)\n",
        "  c = (dual_cof*similarity_mat).sum() + rbf_svm[1].intercept_[0]\n",
        "  cv_class.append(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VusBXPczdOtJ",
        "outputId": "cf78dfaa-dd58-4f9a-a828-746995db6bc4"
      },
      "source": [
        "rbf_svm[1].intercept_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.47845689])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-0zTXg5B9AI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gI6WW78TwqF",
        "outputId": "b38b9f39-26bd-4a2d-e04b-d8bac511d57f"
      },
      "source": [
        "np.count_nonzero(np.sign(rbf_svm[1].decision_function(Xcv)) == np.sign(np.array(cv_class)))/len(np.sign(rbf_svm[1].decision_function(Xcv)) == np.sign(np.array(cv_class)))\n",
        "\n",
        "(rbf_svm[1].decision_function(Xcv)) - (np.array(cv_class)) < 1e-7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejAEe53ezMHE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0bKCboN4Zxu"
      },
      "source": [
        "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMn7OEN94Zxw"
      },
      "source": [
        "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
        "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0n5EFkx4Zxz"
      },
      "source": [
        "## TASK F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0HOqVJq4Zx1"
      },
      "source": [
        "\n",
        "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
        "\n",
        "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
        "\n",
        "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
        "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
        "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
        "\n",
        "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2BO5C37ucQN",
        "outputId": "66ef7a0d-78ea-4a33-8bfc-b9d79cfd9e50"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "ytrain_count=Counter(ytrain)\n",
        "ycv_count = Counter(ycv)\n",
        "print(ytrain_count)\n",
        "print(ycv_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({-1: 2078, 1: 922})\n",
            "Counter({-1: 697, 1: 303})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLBE6HzszUVe"
      },
      "source": [
        "def sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    # compute sigmoid(z) and return\n",
        "    sig = 1/(1+np.exp(-z))\n",
        "    return sig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tixUqm_Loo4x",
        "outputId": "1273356a-58cf-4f6f-d8ff-defd2b88d425"
      },
      "source": [
        "nplus = ycv_count[1]\n",
        "nminus= ycv_count[-1]\n",
        "\n",
        "#sum of yplus and yminus = 1 , they are kind of probabilities.\n",
        "yplus = (nplus + 1)/(nplus + 2) \n",
        "yminus = 1/(nminus+2)\n",
        "\n",
        "\"\"\"ycv_modified = ycv\n",
        "ycv_modified[ycv_modified == -1] = yminus\n",
        "ycv_modified[ycv_modified == 1] = yplus\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ycv_modified = ycv\\nycv_modified[ycv_modified == -1] = yminus\\nycv_modified[ycv_modified == 1] = yplus'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AulWZkNjyxFt"
      },
      "source": [
        "import copy\n",
        "\n",
        "ynew = copy.deepcopy(ycv) #making a deep copy of ycv. In this copy , we replace +1 with yplus and yminus\n",
        "ynew = ynew.astype('float') #how could an array with 'int' data-types ever take in a float value\n",
        "\n",
        "ynew[ynew==-1.0 ] = yminus\n",
        "ynew[ynew==1.0 ] = yplus\n",
        "\n",
        "fcv = np.array(cv_class)#cv_class is a list which was the answer of 8E. It is equivalent to clf.decision_function(Xcv)\n",
        "#fcv will act as training_X and ynew will be training_y for the model by which we wish to predict the calibrated probabilities, which we are interested in \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDgkaKJt58i0"
      },
      "source": [
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "    '''In this function, we will compute the gardient w.r.to w '''\n",
        "    z = np.dot(w,x) + b\n",
        "    dw = x*(y-sigmoid(z)) - alpha*w/N\n",
        "    return dw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3B2hPmy4Pwt"
      },
      "source": [
        "def initialize_weights(dim):\n",
        "    ''' In this function, we will initialize our weights and bias'''\n",
        "    #initialize the weights to zeros array of (1,dim) dimensions\n",
        "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
        "    #initialize bias to zero\n",
        "    w = np.zeros_like(dim)\n",
        "    b = 0.0\n",
        "\n",
        "    return w,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNlHCNqQ6og7"
      },
      "source": [
        " def gradient_db(x,y,w,b):\n",
        "     '''In this function, we will compute gradient w.r.to b '''\n",
        "     z = np.dot(w,x) + b\n",
        "     db = y - sigmoid(z)\n",
        "     return db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL8Jt7mU4ZiX"
      },
      "source": [
        "#I have taken this logoss function from my earlier done assignment and modified it\n",
        "#the logic is for an epoch, after updating weight and calculating predicted probabilities, we send this list as y_pred and ynew will be sent as y_true\n",
        "# I have added yminus and yplus , so this algo can be used for other sets too\n",
        "def logloss(y_true,y_pred, yminus, yplus):\n",
        "    '''In this function, we will compute log loss '''\n",
        "    #y_pred is nothing but a bounded relative measure of distance from the supposed plane(of Log Regrn(ws and b)). \n",
        "    #where measure like 0.99 means a pt far away from the plane in the direction of label(1), 0.002 means a pt far away from the plane in the direction of label(0)\n",
        "    #and 0.5 means the pt is on the plane\n",
        "    #log-loss penalises on 2 bases: 1. the farther away is good, so distance-measure like 0.99 an 0.002 ought to be penalised less. It is like \n",
        "    #\"do you stay in north or south of equator?\" The farther away you are from the equator surer you are to be not confused.[think of visiting kenya]\n",
        "    #2. However, higher distance must be penalised when the device misclassified the one direction[South or North]\n",
        "    #another way to look at it is that the sigmoid is probability of a point being 1 . we know that log-loss fn respects the respective probabs according to their labels\n",
        "    #if a pt has probab of 0.03 for being classified as 1 , it is the same saying it is 0.97 probable to be classiffied as 0.\n",
        "    #i.e, log-loss only once look at the label-value to understand which language too respect\n",
        "\n",
        "    #we use this logic to make the code simpler as following\n",
        "\n",
        "    loss_list = []\n",
        "    for y, pred in zip(y_true, y_pred):\n",
        "      each_loss = -(y*np.log10(pred) + (1-y)*np.log10(1-pred))\n",
        "      loss_list.append(each_loss)\n",
        "      \n",
        "    loss = np.array(loss_list).mean()\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofyo2mJ_rAcg"
      },
      "source": [
        "def train(X_train,y_train,epochs,alpha,eta0,yminus, yplus, tol =1e-4):\n",
        "    ''' In this function, we will implement logistic regression'''\n",
        "    #Here eta0 is learning rate\n",
        "    #implement the code as follows\n",
        "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
        "    # for every epoch\n",
        "        # for every data point(X_train,y_train)                => doubt : isn't it too methodical to be called stochastical? where is randomness? \n",
        "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
        "           #compute gradient w.r.to b (call the gradient_db() function)\n",
        "           #update w, b\n",
        "        # predict the output of x_train[for all data points in X_train] using w,b\n",
        "        #compute the loss between predicted and actual values (call the loss function)=>doubt: \n",
        "        # store all the train loss values in a list\n",
        "        # predict the output of x_test[for all data points in X_test] using w,b\n",
        "        #compute the loss between predicted and actual values (call the loss function)\n",
        "        # store all the test loss values in a list\n",
        "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
        "    \n",
        "    \n",
        "    train_loss_lst = []\n",
        "    \n",
        "    N = len(X_train)\n",
        "    dim = X_train[0]\n",
        "    w,b = initialize_weights(dim)\n",
        "    cond = True\n",
        "    #for iter in range(epochs):#if we assume that one epoch corresponds to one distinct training,(which makes sense, as we have seen all the training points,one by one) then only it is advisible to maintain a list of test losses\n",
        "    num_epoch = 0\n",
        "    while cond:\n",
        "      #print(num_epoch)\n",
        "      for  ind in range(N):\n",
        "      #ind = np.random.randint(low = 0,high = len(X_train),size=1)[0]\n",
        "        x = X_train[ind]\n",
        "        #print(x.shape)\n",
        "        y = y_train[ind]\n",
        "        #print(y.shape)\n",
        "        \n",
        "        dw = gradient_dw(x,y,w,b,alpha,N) #note : logloss + reg are being used for every point , indirectly, by calculating dL/dw and dL/db,\n",
        "        db = gradient_db(x,y,w,b)         #i.e we are taking care of finding the slope and use that to obtain new position and it is actually slope and not the exact value of the terrain_height which gives optimum value of the position we should take, no matter that optimity in first place is defined by the minimity of terrain_height only\n",
        "        w = w + eta0*dw\n",
        "        b = b + eta0*db\n",
        "        \n",
        "      preds = [ sigmoid(np.dot(w,x_row) + b) for x_row in X_train ]\n",
        "\n",
        "\n",
        "      #print(preds)\n",
        "      #print(len(preds[0])) \n",
        "      loss = logloss(y_train, preds,yminus,yplus)\n",
        "      train_loss_lst.append(loss)\n",
        "        \n",
        "      \n",
        "      num_epoch+=1\n",
        "      #cond = not( (len(test_loss_lst)>3 and (test_loss_lst[-2] - test_loss_lst[-1]) >0 and (test_loss_lst[-2] - test_loss_lst[-1]) < tol) or  num_epoch == epochs-1 )\n",
        "      cond = not( (len(train_loss_lst)>3 and (train_loss_lst[-2] - train_loss_lst[-1]) >0 and (train_loss_lst[-2] - train_loss_lst[-1]) < tol) or  num_epoch == epochs-1 )\n",
        "      #print(cond)\n",
        "\n",
        "    return w,b,train_loss_lst,preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11VxN9dNxDk2"
      },
      "source": [
        "alpha=0.0001\n",
        "eta0=0.0001\n",
        "\n",
        "epochs= 30\n",
        "w,b,train_loss_lst,preds=train(fcv,ynew,epochs,alpha,eta0,yminus,yplus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "XIy3JN1V7SMl",
        "outputId": "74740199-d4bf-4aad-b867-572d9efc03e9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(fcv,preds)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTY7z2bd4Zx2"
      },
      "source": [
        "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM3odN1Z4Zx3"
      },
      "source": [
        "\n",
        "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
        "\n",
        "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
        "\n",
        "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
        "\n",
        "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
        "\n",
        "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
      ]
    }
  ]
}